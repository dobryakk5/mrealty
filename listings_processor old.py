import re, json
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook,load_workbook
from openpyxl.styles import Font
from openpyxl.utils import get_column_letter
from io import BytesIO
from typing import List, Dict, Any, Tuple
import asyncio
from datetime import datetime


# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
from db_handler import save_listings, find_similar_ads_grouped, call_update_ad

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º–∏
from photo_processor import PhotoProcessor

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤
HEADERS = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/115.0.0.0 Safari/537.36'
    ),
    'Accept-Language': 'ru-RU,ru;q=0.9',
}

class ListingsProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.photo_processor = PhotoProcessor()
    
    def extract_photo_urls(self, soup: BeautifulSoup) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–∑ –≥–∞–ª–µ—Ä–µ–∏ CIAN"""
        photo_urls = []
        
        try:
            # –ò—â–µ–º –≥–∞–ª–µ—Ä–µ—é –ø–æ data-name="GalleryInnerComponent"
            gallery = soup.find('div', {'data-name': 'GalleryInnerComponent'})
            if not gallery:
                return photo_urls
            
            # –ò—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –≥–∞–ª–µ—Ä–µ–µ
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = gallery.find_all('img', src=True)
            for img in images:
                src = img.get('src')
                if src and src.startswith('http') and 'cdn-cian.ru' in src:
                    photo_urls.append(src)
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ background-image (–¥–ª—è –≤–∏–¥–µ–æ –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ñ–æ—Ç–æ)
            elements_with_bg = gallery.find_all(style=re.compile(r'background-image'))
            for elem in elements_with_bg:
                style = elem.get('style', '')
                bg_match = re.search(r'background-image:\s*url\(["\']?([^"\')\s]+)["\']?\)', style)
                if bg_match:
                    bg_url = bg_match.group(1)
                    if bg_url.startswith('http') and ('cdn-cian.ru' in bg_url or 'kinescopecdn.net' in bg_url):
                        photo_urls.append(bg_url)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
            seen = set()
            unique_photos = []
            for url in photo_urls:
                if url not in seen:
                    seen.add(url)
                    unique_photos.append(url)
            
            return unique_photos
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: {e}")
            return []
    
    async def extract_photo_urls_from_url(self, url: str) -> list[str]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç URL –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, lambda: requests.get(url, headers=HEADERS))
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            return self.extract_photo_urls(soup)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ URL {url}: {e}")
            return []
    
    def extract_listing_info(self, url: str) -> dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
        try:
            response = requests.get(url, headers=HEADERS)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            listing_data = parse_listing(url, requests.Session())
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            info = {
                'rooms': listing_data.get('–ö–æ–º–Ω–∞—Ç', 'N/A'),
                'price': listing_data.get('–¶–µ–Ω–∞_raw', 'N/A'),
                'floor': listing_data.get('–≠—Ç–∞–∂', 'N/A'),
                'total_area': listing_data.get('–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å', 'N/A'),
                'kitchen_area': listing_data.get('–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏', 'N/A'),
                'metro': listing_data.get('–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ', 'N/A')
            }
            
            return info
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏ {url}: {str(e)}")
            return {
                'rooms': 'N/A',
                'price': 'N/A',
                'floor': 'N/A',
                'total_area': 'N/A',
                'kitchen_area': 'N/A',
                'metro': 'N/A'
            }
    
    def generate_html_gallery(self, listing_urls: list[str], user_id: int, subtitle: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –≥–∞–ª–µ—Ä–µ—é —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏"""
        html_parts = []
        
        html_parts.append("""
        <!DOCTYPE html>
        <html lang="ru">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>–ü–æ–¥–±–æ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
                .listing { background: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
                .listing h3 { color: #333; margin-top: 0; margin-bottom: 15px; }
                .listing p { margin: 8px 0; color: #555; }
                .listing strong { color: #333; }
                .main-title { color: #333; margin-bottom: 10px; }
                .subtitle { color: #666; font-size: 18px; margin-bottom: 30px; font-style: italic; }
                .photo-grid { 
                    display: grid; 
                    grid-template-columns: repeat(auto-fill, minmax(180px, 1fr)); 
                    gap: 8px; 
                    margin: 15px 0; 
                    max-height: 600px; 
                    overflow-y: auto; 
                    padding: 10px;
                    border: 1px solid #eee;
                    border-radius: 8px;
                }
                .photo-item { position: relative; }
                .photo-item img { 
                    width: 100%; 
                    height: 140px; 
                    object-fit: cover; 
                    border-radius: 5px; 
                    border: 2px solid transparent;
                    transition: border-color 0.2s;
                }
                .photo-item img:hover { 
                    border-color: #0066cc;
                }
                .no-photos { color: #666; font-style: italic; }
                
                /* –ú–æ–±–∏–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è */
                @media (max-width: 768px) {
                    body { margin: 10px; }
                    .listing { padding: 15px; margin: 15px 0; }
                    .photo-grid { 
                        grid-template-columns: repeat(auto-fill, minmax(150px, 1fr)); 
                        gap: 6px; 
                        padding: 8px;
                    }
                    .photo-item img { 
                        height: 120px; 
                    }
                    .main-title { font-size: 24px; }
                    .subtitle { font-size: 16px; }
                }
            </style>
        </head>
        <body>
            <h1 class="main-title">üè† –ü–æ–¥–±–æ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏</h1>
        """)
        
        if subtitle:
            html_parts.append(f'<h2 class="subtitle">{subtitle}</h2>')
        
        html_parts.append("")
        
        for i, listing_url in enumerate(listing_urls, 1):
            try:
                # –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                listing_data = parse_listing(listing_url, requests.Session())
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
                soup = BeautifulSoup(requests.get(listing_url, headers=HEADERS).text, 'html.parser')
                photo_urls = self.extract_photo_urls(soup)
                
                html_parts.append(f"""
                <div class="listing">
                    <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
                """)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                if '–ö–æ–º–Ω–∞—Ç' in listing_data and listing_data['–ö–æ–º–Ω–∞—Ç']:
                    html_parts.append(f"<p><strong>–ö–æ–º–Ω–∞—Ç:</strong> {listing_data['–ö–æ–º–Ω–∞—Ç']}</p>")
                if '–¶–µ–Ω–∞_raw' in listing_data and listing_data['–¶–µ–Ω–∞_raw']:
                    html_parts.append(f"<p><strong>–¶–µ–Ω–∞:</strong> {listing_data['–¶–µ–Ω–∞_raw']:,} ‚ÇΩ</p>")
                
                # –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç–∞–∂/—ç—Ç–∞–∂–Ω–æ—Å—Ç—å
                if '–≠—Ç–∞–∂' in listing_data and listing_data['–≠—Ç–∞–∂']:
                    html_parts.append(f"<p><strong>–≠—Ç–∞–∂:</strong> {listing_data['–≠—Ç–∞–∂']}</p>")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∞–∂ –æ–±—â–∏–π
                if '–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å' in listing_data and listing_data['–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å']:
                    html_parts.append(f"<p><strong>–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å:</strong> {listing_data['–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å']} –º¬≤</p>")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫—É—Ö–Ω—é
                if '–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏' in listing_data and listing_data['–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏']:
                    html_parts.append(f"<p><strong>–ö—É—Ö–Ω—è:</strong> {listing_data['–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏']} –º¬≤</p>")
                
                # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º "–ú–µ—Ç—Ä–æ" –≤ "–ú–∏–Ω—É—Ç –¥–æ –º–µ—Ç—Ä–æ"
                if '–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ' in listing_data and listing_data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ']:
                    html_parts.append(f"<p><strong>–ú–∏–Ω—É—Ç –¥–æ –º–µ—Ç—Ä–æ:</strong> {listing_data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ']}</p>")
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
                if photo_urls:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–µ—Ç–∫—É —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (–≤—Å–µ —Ñ–æ—Ç–æ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
                    html_parts.append(f'<div class="photo-grid">')
                    for j, photo_url in enumerate(photo_urls):
                        html_parts.append(f"""
                        <div class="photo-item">
                            <img src="{photo_url}" alt="–§–æ—Ç–æ {j+1}" 
                                 onerror="this.style.display='none'; this.nextElementSibling.style.display='block';"
                                 loading="lazy">
                            <div class="photo-fallback" style="display: none; background: #f0f0f0; border: 1px dashed #ccc; border-radius: 5px; padding: 20px; text-align: center; color: #666;">
                                <div>üì∑ –§–æ—Ç–æ {j+1}</div>
                                <div style="font-size: 12px; margin-top: 5px;">
                                    <a href="{photo_url}" target="_blank" style="color: #0066cc;">–û—Ç–∫—Ä—ã—Ç—å —Ñ–æ—Ç–æ</a>
                                </div>
                            </div>
                        </div>
                        """)
                    html_parts.append('</div>')
                else:
                    html_parts.append('<p class="no-photos">üì∑ –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã</p>')
                
                html_parts.append('</div>')
                
            except Exception as e:
                html_parts.append(f"""
                <div class="listing">
                    <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
                    <p style="color: red;">–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}</p>
                </div>
                """)
        
        html_parts.append("""
        </body>
        </html>
        """)
        
        return ''.join(html_parts)
    
    async def generate_html_gallery_embedded(self, listing_urls: list[str], user_id: int, subtitle: str = None, remove_watermarks: bool = False) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –≥–∞–ª–µ—Ä–µ—é —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ Base64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏"""
        html_content = f"""
        <!DOCTYPE html>
        <html lang="ru">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>–ü–æ–¥–±–æ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏</title>
            <style>
                body {{ 
                    font-family: Arial, sans-serif; 
                    margin: 20px; 
                    background-color: #f5f5f5; 
                }}
                .listing {{ 
                    background: white; 
                    margin: 20px 0; 
                    padding: 20px; 
                    border-radius: 10px; 
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1); 
                }}
                .listing h3 {{ 
                    color: #333; 
                    margin-top: 0; 
                    margin-bottom: 15px; 
                }}
                .listing p {{ 
                    margin: 8px 0; 
                    color: #555; 
                }}
                .listing strong {{ 
                    color: #333; 
                }}
                .main-title {{ 
                    color: #333; 
                    margin-bottom: 10px; 
                }}
                .subtitle {{ 
                    color: #666; 
                    font-size: 18px; 
                    margin-bottom: 30px; 
                    font-style: italic; 
                }}
                .photo-grid {{ 
                    display: grid; 
                    grid-template-columns: repeat(auto-fill, minmax(180px, 1fr)); 
                    gap: 8px; 
                    margin: 15px 0; 
                    max-height: 600px; 
                    overflow-y: auto; 
                    padding: 10px;
                    border: 1px solid #eee;
                    border-radius: 8px;
                }}
                .photo-item {{ 
                    position: relative; 
                }}
                .photo-item img {{ 
                    width: 100%; 
                    height: 140px; 
                    object-fit: cover; 
                    border-radius: 5px; 
                    border: 2px solid transparent;
                    transition: border-color 0.2s;
                    background: #f8f9fa;
                }}
                .photo-item img:hover {{ 
                    border-color: #0066cc;
                }}
                .photo-fallback {{ 
                    width: 100%; 
                    height: 140px; 
                    display: flex; 
                    flex-direction: column; 
                    justify-content: center; 
                    align-items: center;
                }}
                .no-photos {{ 
                    color: #666; 
                    font-style: italic; 
                }}
                .photo-info {{ 
                    background: #f8f9fa; 
                    padding: 15px; 
                    border-radius: 8px; 
                    margin-top: 15px; 
                    font-size: 14px;
                    border-left: 4px solid #0066cc;
                }}
                .photo-info strong {{ 
                    color: #333; 
                }}
                .photo-info small {{ 
                    line-height: 1.4; 
                }}
                
                /* –ú–æ–±–∏–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è */
                @media (max-width: 768px) {{
                    body {{ margin: 10px; }}
                    .listing {{ padding: 15px; margin: 15px 0; }}
                    .photo-grid {{ 
                        grid-template-columns: repeat(auto-fill, minmax(150px, 1fr)); 
                        gap: 6px; 
                        padding: 8px;
                    }}
                    .photo-item img, .photo-fallback {{ 
                        height: 120px; 
                    }}
                    .main-title {{ font-size: 24px; }}
                    .subtitle {{ font-size: 16px; }}
                }}
            </style>
        </head>
        <body>
            <h1 class="main-title">üè† –ü–æ–¥–±–æ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏</h1>
        """
        
        if subtitle:
            html_content += f'<h2 class="subtitle">{subtitle}</h2>'
        
        html_content += """
        """
        
        for i, listing_url in enumerate(listing_urls, 1):
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ç–æ –¥–ª—è —ç—Ç–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                photo_urls = await self.extract_photo_urls_from_url(listing_url)
                
                if photo_urls:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–æ—Ç–æ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
                    if remove_watermarks:
                        processed_photos = self.photo_processor.process_photos_for_embedded_html(photo_urls, remove_watermarks=True)
                    else:
                        processed_photos = self.photo_processor.process_photos_for_embedded_html(photo_urls, remove_watermarks=False)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏
                    listing_info = self.extract_listing_info(listing_url)
                    
                    html_content += f"""
                    <div class="listing">
                        <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
                        <p><strong>–ö–æ–º–Ω–∞—Ç:</strong> {listing_info.get('rooms', 'N/A')}</p>
                        <p><strong>–¶–µ–Ω–∞:</strong> {listing_info.get('price', 'N/A')}</p>
                        <p><strong>–≠—Ç–∞–∂:</strong> {listing_info.get('floor', 'N/A')}</p>
                        <p><strong>–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å:</strong> {listing_info.get('total_area', 'N/A')}</p>
                        <p><strong>–ö—É—Ö–Ω—è:</strong> {listing_info.get('kitchen_area', 'N/A')}</p>
                        <p><strong>–ú–∏–Ω—É—Ç –¥–æ –º–µ—Ç—Ä–æ:</strong> {listing_info.get('metro', 'N/A')}</p>
                        <div class="photo-grid">
                    """
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–æ—Ç–æ
                    for j, photo_data in enumerate(processed_photos, 1):
                        if photo_data and 'base64' in photo_data:
                            html_content += f"""
                            <div class="photo-item">
                                <img src="data:image/{photo_data['format']};base64,{photo_data['base64']}" 
                                     alt="–§–æ—Ç–æ {j}" 
                                     loading="lazy">
                            </div>
                            """
                    
                    html_content += f"""
                        </div>
                    </div>
                    """
                else:
                    html_content += f"""
                    <div class="listing">
                        <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
                        <p>–§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã</p>
                    </div>
                    """
                    
            except Exception as e:
                html_content += f"""
                <div class="listing">
                    <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
                    <p>–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}</p>
                </div>
                """
        
        html_content += """
        </body>
        </html>
        """
        
        return html_content

def extract_number(text: str):
    if not text or text == '‚Äî':
        return None
    cleaned = re.sub(r"[^\d.,]", "", text)
    cleaned = cleaned.replace('\u00A0', '').replace(' ', '').replace(',', '.')
    try:
        return float(cleaned) if '.' in cleaned else int(cleaned)
    except ValueError:
        return None

async def export_listings_to_excel(listing_urls: list[str], user_id: int, output_path: str = None) -> tuple[BytesIO, int]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –ë–î –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Excel-—Ñ–∞–π–ª –∏ request_id.
    :param listing_urls: —Å–ø–∏—Å–æ–∫ URL –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    :param user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
    :param output_path: –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –Ω–∞ –¥–∏—Å–∫
    :return: tuple (BytesIO —Å –¥–∞–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞, request_id)
    """
    sess = requests.Session()
    rows = []
    for url in listing_urls:
        try:
            rows.append(parse_listing(url, sess))
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –ø–æ–ª—É—á–∞–µ–º request_id
    request_id = await save_listings(rows, user_id)

    # –§–æ—Ä–º–∏—Ä—É–µ–º DataFrame
    df = pd.DataFrame(rows)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω
    if '–¶–µ–Ω–∞_raw' in df.columns:
        df['–¶–µ–Ω–∞'] = df['–¶–µ–Ω–∞_raw']
        df = df.sort_values('–¶–µ–Ω–∞_raw')
        df.drop('–¶–µ–Ω–∞_raw', axis=1, inplace=True)

    # –ü–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
    ordered = [
        '–ö–æ–º–Ω–∞—Ç', '–¶–µ–Ω–∞', '–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å', '–ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å',
        '–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏', '–°–∞–Ω—É–∑–µ–ª', '–ë–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è', '–í–∏–¥ –∏–∑ –æ–∫–æ–Ω',
        '–†–µ–º–æ–Ω—Ç', '–≠—Ç–∞–∂', '–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏', '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è',
        '–¢–∏–ø –¥–æ–º–∞', '–¢–∏–ø –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤', '–ü–∞—Ä–∫–æ–≤–∫–∞',
        '–ü–æ–¥—ä–µ–∑–¥—ã', '–û—Ç–æ–ø–ª–µ–Ω–∏–µ', '–ê–≤–∞—Ä–∏–π–Ω–æ—Å—Ç—å', '–ì–∞–∑–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ',
        '–í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤', '–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è', '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤',
        '–ê–¥—Ä–µ—Å', '–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ', '–ú–µ—Ç–∫–∏', '–°—Ç–∞—Ç—É—Å', '–¢–∏–ø –∂–∏–ª—å—è', 'URL'
    ]
    df = df[[c for c in ordered if c in df.columns]]

    # –ó–∞–ø–∏—Å—å –≤ BytesIO
    bio = BytesIO()
    df.to_excel(bio, index=False)
    bio.seek(0)

    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å—Ç–æ–ª–±–µ—Ü '–¶–µ–Ω–∞'
    if output_path:
        with open(output_path, 'wb') as f:
            f.write(bio.getbuffer())

        wb = load_workbook(output_path)
        ws = wb.active
        # –í—ã–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        for cell in ws[1]:
            cell.font = Font(bold=True)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É '–¶–µ–Ω–∞' –∏ –∑–∞–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç —Ç—ã—Å—è—á
        price_idx = df.columns.get_loc('–¶–µ–Ω–∞') + 1
        price_col = get_column_letter(price_idx)
        custom_format = '#,##0'
        for row in range(2, ws.max_row + 1):
            cell = ws[f"{price_col}{row}"]
            if isinstance(cell.value, (int, float)):
                cell.number_format = custom_format

        wb.save(output_path)

    return bio, request_id

# –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è
def parse_listing(url: str, session: requests.Session) -> dict:
    resp = session.get(url, headers=HEADERS)
    resp.encoding = 'utf-8'
    soup = BeautifulSoup(resp.text, 'html.parser')

    data = {'URL': url}
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—É—é –±–ª–æ–∫–∏—Ä–æ–≤–∫—É (–∫–∞–ø—á–∞/–∞–Ω—Ç–∏–±–æ—Ç)
    page_text = soup.get_text(" ", strip=True).lower()
    is_blocked = bool(re.search(r"–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –∑–∞–ø—Ä–æ—Å—ã.*–Ω–µ —Ä–æ–±–æ—Ç|–ø–æ—Ö–æ–∂–∏ –Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ", page_text))
    if is_blocked:
        data['–°—Ç–∞—Ç—É—Å'] = None
    elif soup.find(string=re.compile(r"–û–±—ä—è–≤–ª–µ–Ω–∏–µ —Å–Ω—è—Ç–æ", re.IGNORECASE)):
        data['–°—Ç–∞—Ç—É—Å'] = '–°–Ω—è—Ç–æ'
    labels = [span.get_text(strip=True) for span in soup.select('div[data-name="LabelsLayoutNew"] > span span:last-of-type')]
    data['–ú–µ—Ç–∫–∏'] = '; '.join(labels) if labels else None
    h1 = soup.find('h1')
    if h1:
        m = re.search(r"(\d+)[^\d]*[-‚Äì]?–∫–æ–º–Ω", h1.get_text())
        if m:
            data['–ö–æ–º–Ω–∞—Ç'] = extract_number(m.group(1))
    price_el = (
        soup.select_one('[data-name="NewbuildingPriceInfo"] [data-testid="price-amount"] span')
        or soup.select_one('[data-name="AsideGroup"] [data-testid="price-amount"] span')
    )
    if price_el:
        data['–¶–µ–Ω–∞_raw'] = extract_number(price_el.get_text())
        # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –µ—â—ë –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω –∏ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ü–µ–Ω—É ‚Äî —Å—á–∏—Ç–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã–º
        if '–°—Ç–∞—Ç—É—Å' not in data or data['–°—Ç–∞—Ç—É—Å'] is None:
            data['–°—Ç–∞—Ç—É—Å'] = '–ê–∫—Ç–∏–≤–Ω–æ'

    summary = soup.select_one('[data-name="OfferSummaryInfoLayout"]')
    if summary:
        for item in summary.select('[data-name="OfferSummaryInfoItem"]'):
            ps = item.find_all('p')
            if len(ps) < 2:
                continue
            key = ps[0].get_text(strip=True)
            val = ps[1].get_text(strip=True)
            kl = key.lower().strip()
            if key == '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è':
                data[key] = val
                continue
            if kl == '—ç—Ç–∞–∂': data['–≠—Ç–∞–∂'] = val; continue
            if kl in ['—Å–∞–Ω—É–∑–µ–ª', '–±–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤']:
                data[key] = val; continue
            data[key] = extract_number(val) if re.search(r"\d", val) else val

    cont = soup.find('div', {'data-name': 'ObjectFactoids'})
    if cont:
        lines = cont.get_text(separator='\n', strip=True).split('\n')
        for i in range(0, len(lines)-1, 2):
            key, val = lines[i].strip(), lines[i+1].strip()
            kl = key.lower().strip()
            if key == '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è': data[key] = val; continue
            if kl == '—ç—Ç–∞–∂' and '–≠—Ç–∞–∂' not in data: data['–≠—Ç–∞–∂'] = val
            elif kl in ['—Å–∞–Ω—É–∑–µ–ª', '–±–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤']: data[key] = val
            else: data[key] = extract_number(val) if re.search(r"\d", val) else val

    stats_re = re.compile(r"([\d\s]+)\s–ø—Ä–æ—Å–º–æ—Ç—Ä\S*,\s*(\d+)\s–∑–∞ —Å–µ–≥–æ–¥–Ω—è,\s*(\d+)\s—É–Ω–∏–∫–∞–ª—å", re.IGNORECASE)
    st = soup.find(string=stats_re)
    if st:
        m = stats_re.search(st)
        data['–í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'], data['–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è'], data['–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'] = (
            extract_number(m.group(1)), extract_number(m.group(2)), extract_number(m.group(3))
        )

    geo = soup.select_one('div[data-name="Geo"]')
    if geo:
        span = geo.find('span', itemprop='name')
        addr = span['content'] if span and span.get('content') else ', '.join(
            a.get_text(strip=True) for a in geo.select('a[data-name="AddressItem"]')
        )
        parts = [s.strip() for s in addr.split(',') if s.strip()]
        data['–ê–¥—Ä–µ—Å'] = ', '.join(parts[-2:]) if len(parts) > 1 else addr
        stations = []
        for li in geo.select('ul[data-name="UndergroundList"] li[data-name="UndergroundItem"]'):
            st_el = li.find('a', href=True)
            tm_el = li.find('span', class_=re.compile(r".*underground_time.*"))
            if st_el and tm_el:
                name = st_el.get_text(strip=True)
                m = re.search(r"(\d+)", tm_el.get_text(strip=True))
                stations.append((name, int(m.group(1)) if m else None))
        if stations:
            station, time_to = min(stations, key=lambda x: x[1] or float('inf'))
            data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ'] = f"{time_to} {station}"

    return data

def extract_urls(raw_input: str) -> tuple[list[str], int]:
    urls = re.findall(r'https?://[^\s,;]+', raw_input)
    return urls, len(urls)

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
listings_processor = ListingsProcessor()

if __name__ == '__main__':
    user_id = 12345
    urls = ["https://www.cian.ru/sale/flat/318826533/"]
    excel = asyncio.run(export_listings_to_excel(urls, user_id, output_path="listings_numeric.xlsx"))
    print("listings_numeric.xlsx —Å–æ–∑–¥–∞–Ω.")
