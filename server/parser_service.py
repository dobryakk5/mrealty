"""Core parsing logic for the realty parser server."""

from __future__ import annotations

import asyncio
import re
from typing import Any, Dict, List, Optional, Union

import requests
from bs4 import BeautifulSoup

from cian_http_client import fetch_cian_page
from models import PropertyData
from persistent_browser import parse_avito_fast

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä Avito
try:
    from avito_parser_integration import AvitoCardParser

    AVITO_AVAILABLE = True
except ImportError:
    AvitoCardParser = None
    AVITO_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ–¥—É–ª—å avito_parser_integration –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–∞—Ä—Å–∏–Ω–≥ Avito –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä Yandex
try:
    from yandex_parser_integration import YandexCardParser

    YANDEX_AVAILABLE = True
except ImportError:
    YandexCardParser = None
    YANDEX_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ–¥—É–ª—å yandex_parser_integration –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–∞—Ä—Å–∏–Ω–≥ Yandex –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä Baza Winner (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
try:
    from baza_winner_parser import BazaWinnerParser  # noqa: F401

    BAZA_WINNER_AVAILABLE = True
except ImportError:
    BazaWinnerParser = None
    BAZA_WINNER_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ–¥—É–ª—å baza_winner_parser –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–∞—Ä—Å–∏–Ω–≥ Baza Winner –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö
try:
    from extended_data_collector import get_property_by_guid

    EXTENDED_COLLECTOR_AVAILABLE = True
except ImportError:
    get_property_by_guid = None
    EXTENDED_COLLECTOR_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ–¥—É–ª—å extended_data_collector –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ GUID –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ru-RU,ru;q=0.9",
}


class RealtyParserAPI:
    """API –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏."""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def is_avito_url(self, url: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ —Å—Å—ã–ª–∫–æ–π –Ω–∞ Avito."""

        return "avito.ru" in url.lower()

    def is_cian_url(self, url: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ —Å—Å—ã–ª–∫–æ–π –Ω–∞ Cian."""

        return "cian.ru" in url.lower()

    def is_yandex_url(self, url: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ —Å—Å—ã–ª–∫–æ–π –Ω–∞ Yandex Realty."""

        return "realty.yandex.ru" in url.lower() or "realty.ya.ru" in url.lower()

    def _extract_station_from_metro_time(self, metro_time: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏ –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ '6 –¢–µ–∫—Å—Ç–∏–ª—å—â–∏–∫–∏'."""

        if not metro_time or not isinstance(metro_time, str):
            return None

        parts = metro_time.strip().split(" ", 1)
        if len(parts) >= 2 and parts[0].isdigit():
            return parts[1]
        return None

    def _extract_minutes_from_metro_time(self, metro_time: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∏–Ω—É—Ç—ã –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ '6 –¢–µ–∫—Å—Ç–∏–ª—å—â–∏–∫–∏'."""

        if not metro_time or not isinstance(metro_time, str):
            return None

        parts = metro_time.strip().split(" ", 1)
        if len(parts) >= 1 and parts[0].isdigit():
            try:
                return int(parts[0])
            except ValueError:
                return None
        return None

    def get_url_source(self, url: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ —Å—Å—ã–ª–∫–∏."""

        if self.is_avito_url(url):
            return "avito"
        if self.is_cian_url(url):
            return "cian"
        if self.is_yandex_url(url):
            return "yandex"
        return "unknown"

    def _determine_status(self, status_str: Optional[str]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É —Å—Ç–∞—Ç—É—Å—É."""

        if not status_str:
            return True

        status_lower = status_str.lower().strip()

        if status_lower == "inactive":
            return False
        if status_lower == "active":
            return True

        inactive_statuses = [
            "—Å–Ω—è—Ç–æ",
            "–Ω–µ–∞–∫—Ç–∏–≤–Ω–æ",
            "–∞—Ä—Ö–∏–≤",
            "—É–¥–∞–ª–µ–Ω–æ",
            "–ø—Ä–æ–¥–∞–Ω–æ",
            "—Å–¥–∞–Ω–æ",
            "–Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω–æ–µ",
            "–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ",
            "—É—Å—Ç–∞—Ä–µ–ª–æ",
            "inactive",
        ]

        return not any(inactive_status in status_lower for inactive_status in inactive_statuses)

    async def parse_property(self, url: str, skip_photos: bool = True) -> Optional[PropertyData]:
        """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""

        try:
            if self.is_avito_url(url):
                return await self._parse_avito_property(url, skip_photos=skip_photos)
            if self.is_cian_url(url):
                return await self._parse_cian_property(url)
            if self.is_yandex_url(url):
                return await self._parse_yandex_property_quick(url)
            print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —Å—Å—ã–ª–∫–∏: {url}")
            return None
        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {exc}")
            return None

    async def parse_property_extended(self, url: str, skip_photos: bool = True) -> Optional[PropertyData]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""

        try:
            if self.is_avito_url(url):
                return await self._parse_avito_extended(url, skip_photos=skip_photos)
            if self.is_cian_url(url):
                return await self._parse_cian_property(url)
            if self.is_yandex_url(url):
                return await self._parse_yandex_property(url)
            print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —Å—Å—ã–ª–∫–∏: {url}")
            return None
        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {exc}")
            return None

    async def parse_property_flat_state(self, url: str) -> Optional[PropertyData]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–æ–ª—å–∫–æ —Ü–µ–Ω—É, —Å—Ç–∞—Ç—É—Å –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""

        try:
            if self.is_avito_url(url):
                data = await self._parse_avito_extended(url, skip_photos=True)
            elif self.is_cian_url(url):
                data = await self._parse_cian_flat_state(url)
            elif self.is_yandex_url(url):
                data = await self._parse_yandex_property(url)
            else:
                print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —Å—Å—ã–ª–∫–∏ (flat_state): {url}")
                return None

            if data:
                return PropertyData(
                    price=data.price,
                    status=data.status,
                    views_today=data.views_today,
                    url=url,
                )
            return None
        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ flat_state –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {exc}")
            return None

    async def parse_properties_batch(self, urls: List[str], skip_photos: bool = True) -> List[PropertyData]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π."""

        results: List[PropertyData] = []
        for i, url in enumerate(urls, 1):
            try:
                print(f"üîÑ –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {i}/{len(urls)}: {url}")
                property_data = await self.parse_property(url, skip_photos=skip_photos)
                if property_data:
                    results.append(property_data)
                    print("‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
                else:
                    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
            except Exception as exc:  # noqa: BLE001
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {exc}")
                continue

        print(f"üìä –í—Å–µ–≥–æ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)} –∏–∑ {len(urls)}")
        return results

    async def _parse_avito_light(self, url: str) -> Optional[PropertyData]:
        """–õ–µ–≥–∫–∏–π –ø–∞—Ä—Å–µ—Ä Avito —á–µ—Ä–µ–∑ persistent –±—Ä–∞—É–∑–µ—Ä."""

        try:
            print(f"üîç –õ–µ–≥–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ Avito (persistent): {url}")
            data = parse_avito_fast(url)

            if data:
                title = data.get("title", "")
                h1 = data.get("h1", "")
                price_text = data.get("price", "")
                parsed_data = self._extract_data_from_title(title, h1)

                if "rooms" in data:
                    parsed_data["rooms"] = data["rooms"]
                if "total_area" in data:
                    parsed_data["total_area"] = data["total_area"]
                if "floor" in data:
                    parsed_data["floor"] = data["floor"]
                if "total_floors" in data:
                    parsed_data["total_floors"] = data["total_floors"]

                price = None
                if price_text:
                    price_match = re.search(r"(\d[\d\s]*)", price_text.replace("\u00a0", " "))
                    if price_match:
                        price_str = price_match.group(1).replace(" ", "")
                        try:
                            price = float(price_str)
                        except Exception:
                            pass

                if parsed_data:
                    has_rooms = parsed_data.get("rooms") is not None
                    status = has_rooms

                    print(f"üìä –°—Ç–∞—Ç—É—Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {'–∞–∫—Ç–∏–≤–Ω–æ' if status else '–Ω–µ–∞–∫—Ç–∏–≤–Ω–æ'} (–∫–æ–º–Ω–∞—Ç—ã: {parsed_data.get('rooms')})")

                    return PropertyData(
                        rooms=parsed_data.get("rooms"),
                        price=price,
                        total_area=parsed_data.get("total_area"),
                        floor=parsed_data.get("floor"),
                        total_floors=parsed_data.get("total_floors"),
                        source="avito",
                        url=url,
                        status=status,
                    )
            return None

        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ –ª–µ–≥–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ (persistent): {exc}")
            return None

    def _extract_data_from_title(self, title: str, h1: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ H1 Avito."""

        try:
            text = h1 if h1 else title
            print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç: {text}")

            data: Dict[str, Any] = {}

            rooms_match = re.search(r"(\d+)-–∫\.", text)
            if rooms_match:
                data["rooms"] = int(rooms_match.group(1))
                print(f"üè† –ö–æ–º–Ω–∞—Ç: {data['rooms']}")

            if re.search(r"\b—Å—Ç—É–¥–∏—è\b|\b–∞–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã\b", text.lower()):
                data["rooms"] = 0
                print("üè† –¢–∏–ø –∂–∏–ª—å—è: —Å—Ç—É–¥–∏—è/–∞–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã (–∫–æ–º–Ω–∞—Ç: 0)")

            area_match = re.search(r"(\d+(?:[.,]\d+)?)\s*–º¬≤", text)
            if area_match:
                area_str = area_match.group(1).replace(",", ".")
                data["total_area"] = float(area_str)
                print(f"üìê –ü–ª–æ—â–∞–¥—å: {data['total_area']} –º¬≤")

            floor_match = re.search(r"(\d+)/(\d+)\s*—ç—Ç\.", text)
            if floor_match:
                data["floor"] = floor_match.group(1)
                data["total_floors"] = int(floor_match.group(2))
                print(f"üè¢ –≠—Ç–∞–∂: {data['floor']}/{data['total_floors']}")

            return data

        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {exc}")
            return {}

    async def _parse_avito_property(self, url: str, skip_photos: bool = True) -> Optional[PropertyData]:
        """–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å Avito (—Ç–æ–ª—å–∫–æ –ª–µ–≥–∫–∏–π –ø–∞—Ä—Å–µ—Ä)."""

        return await self._parse_avito_light(url)

    async def _parse_avito_extended(self, url: str, skip_photos: bool = True) -> Optional[PropertyData]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å Avito (–ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)."""

        if not AVITO_AVAILABLE or AvitoCardParser is None:
            print("‚ùå –ü–∞—Ä—Å–µ—Ä Avito –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return None

        try:
            print(f"üè† –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ Avito (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä): {url}")
            parser = AvitoCardParser(skip_photos=skip_photos)
            parsed_data = parser.parse_avito_page(url)
            if not parsed_data:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Avito")
                return None

            db_data = parser.prepare_data_for_db(parsed_data)
            print("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ Selenium –ø–∞—Ä—Å–∏–Ω–≥–∞")
            if not db_data:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ë–î")
                return None

            price = db_data.get("price")
            is_active = price not in (None, "", 0)

            property_data = PropertyData(
                rooms=db_data.get("rooms"),
                price=price,
                total_area=db_data.get("total_area"),
                living_area=db_data.get("living_area"),
                kitchen_area=db_data.get("kitchen_area"),
                floor=db_data.get("floor"),
                total_floors=db_data.get("total_floors"),
                bathroom=db_data.get("bathroom"),
                balcony=db_data.get("balcony"),
                renovation=db_data.get("renovation"),
                construction_year=db_data.get("construction_year"),
                house_type=db_data.get("house_type"),
                ceiling_height=db_data.get("ceiling_height"),
                furniture=db_data.get("furniture"),
                address=db_data.get("address"),
                metro_station=db_data.get("metro_station"),
                metro_time=db_data.get("metro_time"),
                metro_way=db_data.get("metro_way"),
                tags=db_data.get("tags"),
                description=db_data.get("description"),
                photo_urls=db_data.get("photo_urls"),
                source="avito",
                url=url,
                status=is_active,
                views_today=db_data.get("today_views"),
            )

            print("‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ Avito —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
            return property_data

        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Avito: {exc}")
            return None
        finally:
            if "parser" in locals() and getattr(parser, "driver", None):
                try:
                    parser.cleanup()
                except Exception:
                    pass

    async def _parse_yandex_property_quick(self, url: str) -> Optional[PropertyData]:
        """–ë—ã—Å—Ç—Ä–æ –ø–∞—Ä—Å–∏—Ç —Ü–µ–Ω—É –∏ —Å—Ç–∞—Ç—É—Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å Yandex Realty."""

        if not YANDEX_AVAILABLE or YandexCardParser is None:
            print("‚ùå –ü–∞—Ä—Å–µ—Ä Yandex –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return None

        try:
            print(f"‚ö° –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Yandex Realty: {url}")
            parser = YandexCardParser()
            parsed_data = parser.parse_yandex_quick(url)
            if not parsed_data:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –±—ã—Å—Ç—Ä–æ —Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Yandex Realty")
                return None

            db_data = parser.prepare_quick_data_for_db(parsed_data)
            if not db_data:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –±—ã—Å—Ç—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ë–î")
                return None

            yandex_status = self._determine_status(db_data.get("status"))
            price = db_data.get("price")

            return PropertyData(
                price=price,
                source="yandex",
                url=url,
                status=yandex_status,
            )

        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Yandex Realty: {exc}")
            return None
        finally:
            if "parser" in locals():
                try:
                    parser.cleanup()
                except Exception:
                    pass

    async def _parse_yandex_property(self, url: str) -> Optional[PropertyData]:
        """–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å Yandex Realty (–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)."""

        if not YANDEX_AVAILABLE or YandexCardParser is None:
            print("‚ùå –ü–∞—Ä—Å–µ—Ä Yandex –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return None

        try:
            print(f"üè† –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ Yandex Realty: {url}")
            parser = YandexCardParser()
            parsed_data = parser.parse_yandex_page(url)
            if not parsed_data:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Yandex Realty")
                return None

            db_data = parser.prepare_data_for_db(parsed_data)
            if not db_data:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ë–î")
                return None

            yandex_status = self._determine_status(db_data.get("status"))
            price = db_data.get("price")

            property_data = PropertyData(
                rooms=db_data.get("rooms"),
                price=price,
                total_area=db_data.get("area_total"),
                living_area=db_data.get("living_area"),
                kitchen_area=db_data.get("kitchen_area"),
                floor=db_data.get("floor"),
                total_floors=db_data.get("floor_total"),
                bathroom=db_data.get("bathroom"),
                balcony=db_data.get("balcony"),
                renovation=db_data.get("renovation"),
                construction_year=db_data.get("year_built"),
                house_type=db_data.get("house_type"),
                address=db_data.get("address"),
                metro_station=self._extract_station_from_metro_time(db_data.get("metro_time")),
                metro_time=self._extract_minutes_from_metro_time(db_data.get("metro_time")),
                description=db_data.get("description"),
                source="yandex",
                url=url,
                status=yandex_status,
                views_today=db_data.get("views"),
            )

            print("‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ Yandex Realty —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
            return property_data

        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Yandex Realty: {exc}")
            return None
        finally:
            if "parser" in locals():
                try:
                    parser.cleanup()
                except Exception:
                    pass

    async def _parse_cian_property(self, url: str) -> Optional[PropertyData]:
        """–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å Cian."""

        try:
            print(f"üè† –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ Cian: {url}")

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: fetch_cian_page(
                    url,
                    headers=dict(self.session.headers),
                    cookies=None,
                    proxy=None,
                    timeout=30,
                ),
            )
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            data = self._extract_cian_data(soup, url)

            property_data = PropertyData(
                rooms=data.get("–ö–æ–º–Ω–∞—Ç"),
                price=data.get("–¶–µ–Ω–∞_raw"),
                total_area=data.get("–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å"),
                living_area=data.get("–ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å"),
                kitchen_area=data.get("–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏"),
                floor=data.get("–≠—Ç–∞–∂"),
                total_floors=data.get("–í—Å–µ–≥–æ —ç—Ç–∞–∂–µ–π"),
                bathroom=data.get("–°–∞–Ω—É–∑–µ–ª"),
                balcony=data.get("–ë–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è"),
                renovation=data.get("–†–µ–º–æ–Ω—Ç"),
                construction_year=data.get("–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏"),
                house_type=data.get("–¢–∏–ø –¥–æ–º–∞"),
                ceiling_height=data.get("–í—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤"),
                furniture=data.get("–ú–µ–±–µ–ª—å"),
                address=data.get("–ê–¥—Ä–µ—Å"),
                metro_station=self._extract_station_from_metro_time(data.get("–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ")),
                metro_time=self._extract_minutes_from_metro_time(data.get("–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ")),
                tags=data.get("–ú–µ—Ç–∫–∏"),
                description=data.get("–û–ø–∏—Å–∞–Ω–∏–µ"),
                photo_urls=data.get("photo_urls", []),
                source="cian",
                url=url,
                status=self._determine_status(data.get("–°—Ç–∞—Ç—É—Å", "active")),
                views_today=data.get("–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è"),
            )

            print("‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ Cian —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
            return property_data

        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Cian: {exc}")
            return None

    async def _parse_cian_flat_state(self, url: str) -> Optional[PropertyData]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–æ–ª—å–∫–æ —Ü–µ–Ω—É/—Å—Ç–∞—Ç—É—Å/–ø—Ä–æ—Å–º–æ—Ç—Ä—ã —Å Cian."""

        try:
            print(f"üè† –ü–∞—Ä—Å–∏–º flat_state Cian: {url}")
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: fetch_cian_page(
                    url,
                    headers=dict(self.session.headers),
                    cookies=None,
                    proxy=None,
                    timeout=30,
                ),
            )
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            data = self._extract_cian_data(soup, url)
            price = data.get("–¶–µ–Ω–∞_raw")
            status = self._determine_status(data.get("–°—Ç–∞—Ç—É—Å", "active"))
            views_today = data.get("–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è")

            return PropertyData(
                price=price,
                status=status,
                views_today=views_today,
                source="cian",
                url=url,
            )
        except Exception as exc:  # noqa: BLE001
            print(f"‚ùå –û—à–∏–±–∫–∞ flat_state –ø–∞—Ä—Å–∏–Ω–≥–∞ Cian: {exc}")
            return None

    def _extract_cian_data(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã Cian."""

        data: Dict[str, Any] = {"URL": url}

        page_text = soup.get_text(" ", strip=True).lower()
        is_blocked = bool(re.search(r"–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –∑–∞–ø—Ä–æ—Å—ã.*–Ω–µ —Ä–æ–±–æ—Ç|–ø–æ—Ö–æ–∂–∏ –Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ", page_text))
        if is_blocked:
            data["–°—Ç–∞—Ç—É—Å"] = None
        elif soup.find(string=re.compile(r"–û–±—ä—è–≤–ª–µ–Ω–∏–µ —Å–Ω—è—Ç–æ", re.IGNORECASE)):
            data["–°—Ç–∞—Ç—É—Å"] = "–°–Ω—è—Ç–æ"

        labels: List[str] = []
        label_selectors = [
            'div[data-name="LabelsLayoutNew"] > span[class]',
            'div[data-name="LabelsLayoutNew"] span[data-testid]',
            'div[data-name="LabelsLayoutNew"] span:not(:has(span))',
        ]

        for selector in label_selectors:
            try:
                spans = soup.select(selector)
                if spans:
                    labels = [span.get_text(strip=True) for span in spans if span.get_text(strip=True)]
                    break
            except Exception:
                continue

        data["–ú–µ—Ç–∫–∏"] = "; ".join(labels) if labels else None

        h1 = soup.find("h1")
        if h1:
            match = re.search(r"(\d+)[^\d]*[-‚Äì]?–∫–æ–º–Ω", h1.get_text())
            if match:
                data["–ö–æ–º–Ω–∞—Ç"] = self._extract_number(match.group(1))

        price_el = (
            soup.select_one('[data-name="NewbuildingPriceInfo"] [data-testid="price-amount"] span')
            or soup.select_one('[data-name="AsideGroup"] [data-testid="price-amount"] span')
        )
        if price_el:
            data["–¶–µ–Ω–∞_raw"] = self._extract_number(price_el.get_text())
            if "–°—Ç–∞—Ç—É—Å" not in data or data["–°—Ç–∞—Ç—É—Å"] is None:
                data["–°—Ç–∞—Ç—É—Å"] = "–ê–∫—Ç–∏–≤–Ω–æ"

        summary = soup.select_one('[data-name="OfferSummaryInfoLayout"]')
        if summary:
            for item in summary.select('[data-name="OfferSummaryInfoItem"]'):
                ps = item.find_all("p")
                if len(ps) < 2:
                    continue
                key = ps[0].get_text(strip=True)
                val = ps[1].get_text(strip=True)
                kl = key.lower().strip()
                if key == "–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è":
                    data[key] = val
                    continue
                if kl == "—ç—Ç–∞–∂":
                    floor_info = self._parse_floor_info(val)
                    data["–≠—Ç–∞–∂"] = floor_info["current_floor"]
                    data["–í—Å–µ–≥–æ —ç—Ç–∞–∂–µ–π"] = floor_info["total_floors"]
                    continue
                if kl in ["—Å–∞–Ω—É–∑–µ–ª", "–±–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤"]:
                    data[key] = val
                    continue
                data[key] = self._extract_number(val) if re.search(r"\d", val) else val

        cont = soup.find("div", {"data-name": "ObjectFactoids"})
        if cont:
            lines = cont.get_text(separator="\n", strip=True).split("\n")
            for i in range(0, len(lines) - 1, 2):
                key, val = lines[i].strip(), lines[i + 1].strip()
                kl = key.lower().strip()
                if key == "–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è":
                    data[key] = val
                    continue
                if kl == "—ç—Ç–∞–∂" and "–≠—Ç–∞–∂" not in data:
                    floor_info = self._parse_floor_info(val)
                    data["–≠—Ç–∞–∂"] = floor_info["current_floor"]
                    data["–í—Å–µ–≥–æ —ç—Ç–∞–∂–µ–π"] = floor_info["total_floors"]
                elif kl in ["—Å–∞–Ω—É–∑–µ–ª", "–±–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤"]:
                    data[key] = val
                else:
                    data[key] = self._extract_number(val) if re.search(r"\d", val) else val

        stats_re = re.compile(r"([\d\s]+)\s–ø—Ä–æ—Å–º–æ—Ç—Ä\S*,\s*(\d+)\s–∑–∞ —Å–µ–≥–æ–¥–Ω—è,\s*(\d+)\s—É–Ω–∏–∫–∞–ª—å", re.IGNORECASE)
        stats_text = soup.find(string=stats_re)
        if stats_text:
            match = stats_re.search(stats_text)
            data["–í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"], data["–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è"], data["–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"] = (
                self._extract_number(match.group(1)),
                self._extract_number(match.group(2)),
                self._extract_number(match.group(3)),
            )

        geo = soup.select_one('div[data-name="Geo"]')
        if geo:
            span = geo.find("span", itemprop="name")
            addr = span["content"] if span and span.get("content") else ", ".join(
                a.get_text(strip=True) for a in geo.select('a[data-name="AddressItem"]')
            )
            parts = [s.strip() for s in addr.split(",") if s.strip()]
            data["–ê–¥—Ä–µ—Å"] = ", ".join(parts[-2:]) if len(parts) > 1 else addr

            stations = []
            for li in geo.select('ul[data-name="UndergroundList"] li[data-name="UndergroundItem"]'):
                station_el = li.find("a", href=True)
                time_el = li.find("span", class_=re.compile(r".*underground_time.*"))
                if station_el and time_el:
                    name = station_el.get_text(strip=True)
                    match = re.search(r"(\d+)", time_el.get_text(strip=True))
                    stations.append((name, int(match.group(1)) if match else None))
            if stations:
                station, time_to = min(stations, key=lambda value: value[1] or float("inf"))
                data["–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ"] = f"{time_to} {station}"

        data["photo_urls"] = self._extract_cian_photos(soup)
        return data

    def _parse_floor_info(self, text: str) -> Dict[str, Optional[int]]:
        """–†–∞–∑–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç—Ç–∞–∂–µ –Ω–∞ —Ç–µ–∫—É—â–∏–π –∏ –æ–±—â–∏–π."""

        if not text:
            return {"current_floor": None, "total_floors": None}

        normalized = str(text).replace("\u00A0", " ").strip().lower()
        match = re.search(r"(\d+)\s*(?:–∏–∑|/)\s*(\d+)", normalized)
        if match:
            return {"current_floor": int(match.group(1)), "total_floors": int(match.group(2))}

        match = re.search(r"(\d+)\b", normalized)
        if match:
            return {"current_floor": int(match.group(1)), "total_floors": None}

        return {"current_floor": None, "total_floors": None}

    def _extract_cian_photos(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —Å Cian."""

        photo_urls: List[str] = []

        try:
            gallery = soup.find("div", {"data-name": "GalleryInnerComponent"})
            if not gallery:
                return photo_urls

            images = gallery.find_all("img", src=True)
            for img in images:
                src = img.get("src")
                if src and src.startswith("http") and "cdn-cian.ru" in src:
                    photo_urls.append(src)

            elements_with_bg = gallery.find_all(style=re.compile(r"background-image"))
            for element in elements_with_bg:
                style = element.get("style", "")
                bg_match = re.search(r'background-image:\s*url\(["\']?([^"\')\s]+)["\']?\)', style)
                if bg_match:
                    bg_url = bg_match.group(1)
                    if bg_url.startswith("http") and ("cdn-cian.ru" in bg_url or "kinescopecdn.net" in bg_url):
                        photo_urls.append(bg_url)

            seen = set()
            unique_photos: List[str] = []
            for photo_url in photo_urls:
                if photo_url not in seen:
                    seen.add(photo_url)
                    unique_photos.append(photo_url)

            return unique_photos

        except Exception as exc:  # noqa: BLE001
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π Cian: {exc}")
            return []

    def _extract_number(self, text: str) -> Optional[Union[int, float]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""

        if not text or text == "‚Äî":
            return None
        cleaned = re.sub(r"[^\d.,]", "", text)
        cleaned = cleaned.replace("\u00A0", "").replace(" ", "").replace(",", ".")
        try:
            return float(cleaned) if "." in cleaned else int(cleaned)
        except ValueError:
            return None

    def cleanup(self) -> None:
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã."""

        try:
            self.session.close()
        except Exception as exc:  # noqa: BLE001
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Ä–µ—Å—É—Ä—Å–æ–≤: {exc}")

    def __del__(self):
        self.cleanup()


parser = RealtyParserAPI()


async def parse_property(url: str, skip_photos: bool = True) -> Optional[PropertyData]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""

    return await parser.parse_property(url, skip_photos=skip_photos)


async def parse_property_extended(url: str, skip_photos: bool = True) -> Optional[PropertyData]:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""

    return await parser.parse_property_extended(url, skip_photos=skip_photos)


async def parse_property_flat_state(url: str) -> Optional[PropertyData]:
    """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: —Ü–µ–Ω–∞ + —Å—Ç–∞—Ç—É—Å + –ø—Ä–æ—Å–º–æ—Ç—Ä—ã."""

    return await parser.parse_property_flat_state(url)


async def parse_properties_batch(urls: List[str], skip_photos: bool = True) -> List[PropertyData]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥."""

    return await parser.parse_properties_batch(urls, skip_photos=skip_photos)


def extract_urls(raw_input: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑ —Ç–µ–∫—Å—Ç–∞."""

    return re.findall(r"https?://[^\s,;]+", raw_input)


__all__ = [
    "RealtyParserAPI",
    "AVITO_AVAILABLE",
    "YANDEX_AVAILABLE",
    "BAZA_WINNER_AVAILABLE",
    "EXTENDED_COLLECTOR_AVAILABLE",
    "parser",
    "parse_property",
    "parse_property_extended",
    "parse_properties_batch",
    "extract_urls",
    "get_property_by_guid",
    "parse_property_flat_state",
]
