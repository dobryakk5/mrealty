"""
HTTP API —Å–µ—Ä–≤–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
–ü—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å Fastify (Node.js)
"""

import re
import json
import asyncio
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, asdict
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä Avito
try:
    from avito_parser_integration import AvitoCardParser
    AVITO_AVAILABLE = True
except ImportError:
    AVITO_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ–¥—É–ª—å avito_parser_integration –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–∞—Ä—Å–∏–Ω–≥ Avito –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤
HEADERS = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/115.0.0.0 Safari/537.36'
    ),
    'Accept-Language': 'ru-RU,ru;q=0.9',
}

@dataclass
class PropertyData:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    rooms: Optional[int] = None
    price: Optional[float] = None
    total_area: Optional[float] = None
    living_area: Optional[float] = None
    kitchen_area: Optional[float] = None
    floor: Optional[str] = None
    total_floors: Optional[int] = None
    
    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    bathroom: Optional[str] = None
    balcony: Optional[str] = None
    renovation: Optional[str] = None
    construction_year: Optional[int] = None
    house_type: Optional[str] = None
    ceiling_height: Optional[float] = None
    furniture: Optional[str] = None
    
    # –†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ
    address: Optional[str] = None
    metro_station: Optional[str] = None
    metro_time: Optional[int] = None
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ
    tags: Optional[List[str]] = None
    description: Optional[str] = None
    photo_urls: Optional[List[str]] = None
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    source: Optional[str] = None  # 'avito' –∏–ª–∏ 'cian'
    url: Optional[str] = None
    status: Optional[str] = None
    views_today: Optional[int] = None
    total_views: Optional[int] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è JSON"""
        return asdict(self)
    
    def to_json(self) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ JSON —Å—Ç—Ä–æ–∫—É"""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è FastAPI
class ParseUrlsRequest(BaseModel):
    urls: List[str]

class ParseTextRequest(BaseModel):
    text: str

class ParseResponse(BaseModel):
    success: bool
    data: List[PropertyData]
    total: int
    message: str
    timestamp: str

class RealtyParserAPI:
    """API –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
    
    def is_avito_url(self, url: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ —Å—Å—ã–ª–∫–æ–π –Ω–∞ Avito"""
        return 'avito.ru' in url.lower()
    
    def is_cian_url(self, url: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ —Å—Å—ã–ª–∫–æ–π –Ω–∞ Cian"""
        return 'cian.ru' in url.lower()
    
    def get_url_source(self, url: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ —Å—Å—ã–ª–∫–∏"""
        if self.is_avito_url(url):
            return 'avito'
        elif self.is_cian_url(url):
            return 'cian'
        else:
            return 'unknown'
    
    async def parse_property(self, url: str) -> Optional[PropertyData]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ PropertyData
        """
        try:
            if self.is_avito_url(url):
                return await self._parse_avito_property(url)
            elif self.is_cian_url(url):
                return await self._parse_cian_property(url)
            else:
                print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —Å—Å—ã–ª–∫–∏: {url}")
                return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
            return None
    
    async def parse_properties_batch(self, urls: List[str]) -> List[PropertyData]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ PropertyData –æ–±—ä–µ–∫—Ç–æ–≤
        """
        results = []
        for i, url in enumerate(urls, 1):
            try:
                print(f"üîÑ –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {i}/{len(urls)}: {url}")
                property_data = await self.parse_property(url)
                if property_data:
                    results.append(property_data)
                    print(f"‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
                else:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
                continue
        
        print(f"üìä –í—Å–µ–≥–æ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)} –∏–∑ {len(urls)}")
        return results
    
    async def _parse_avito_property(self, url: str) -> Optional[PropertyData]:
        """–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å Avito"""
        if not AVITO_AVAILABLE:
            print("‚ùå –ü–∞—Ä—Å–µ—Ä Avito –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return None
        
        try:
            print(f"üè† –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ Avito: {url}")
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä Avito
            parser = AvitoCardParser()
            
            # –ü–∞—Ä—Å–∏–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            parsed_data = parser.parse_avito_page(url)
            if not parsed_data:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Avito")
                return None
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ë–î
            db_data = parser.prepare_data_for_db(parsed_data)
            if not db_data:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ë–î")
                return None
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç
            property_data = PropertyData(
                rooms=db_data.get('rooms'),
                price=db_data.get('price'),
                total_area=db_data.get('total_area'),
                living_area=db_data.get('living_area'),
                kitchen_area=db_data.get('kitchen_area'),
                floor=db_data.get('floor'),
                total_floors=db_data.get('total_floors'),
                bathroom=db_data.get('bathroom'),
                balcony=db_data.get('balcony'),
                renovation=db_data.get('renovation'),
                construction_year=db_data.get('construction_year'),
                house_type=db_data.get('house_type'),
                ceiling_height=db_data.get('ceiling_height'),
                furniture=db_data.get('furniture'),
                address=db_data.get('address'),
                metro_station=db_data.get('metro_station'),
                metro_time=db_data.get('metro_time'),
                tags=db_data.get('tags'),
                description=db_data.get('description'),
                photo_urls=db_data.get('photo_urls'),
                source='avito',
                url=url,
                status='active',
                views_today=db_data.get('today_views'),
                total_views=db_data.get('total_views')
            )
            
            print(f"‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ Avito —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
            return property_data
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Avito: {e}")
            return None
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
            if 'parser' in locals() and parser.driver:
                try:
                    parser.cleanup()
                except:
                    pass
    
    async def _parse_cian_property(self, url: str) -> Optional[PropertyData]:
        """–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å Cian"""
        try:
            print(f"üè† –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ Cian: {url}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.session.get(url)
            )
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data = self._extract_cian_data(soup, url)
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç
            property_data = PropertyData(
                rooms=data.get('–ö–æ–º–Ω–∞—Ç'),
                price=data.get('–¶–µ–Ω–∞_raw'),
                total_area=data.get('–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å'),
                living_area=data.get('–ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å'),
                kitchen_area=data.get('–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏'),
                floor=data.get('–≠—Ç–∞–∂'),
                total_floors=data.get('–í—Å–µ–≥–æ —ç—Ç–∞–∂–µ–π'),
                bathroom=data.get('–°–∞–Ω—É–∑–µ–ª'),
                balcony=data.get('–ë–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è'),
                renovation=data.get('–†–µ–º–æ–Ω—Ç'),
                construction_year=data.get('–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏'),
                house_type=data.get('–¢–∏–ø –¥–æ–º–∞'),
                ceiling_height=data.get('–í—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤'),
                furniture=data.get('–ú–µ–±–µ–ª—å'),
                address=data.get('–ê–¥—Ä–µ—Å'),
                metro_station=data.get('–ú–µ—Ç—Ä–æ'),
                metro_time=data.get('–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ'),
                tags=data.get('–ú–µ—Ç–∫–∏'),
                description=data.get('–û–ø–∏—Å–∞–Ω–∏–µ'),
                photo_urls=data.get('photo_urls', []),
                source='cian',
                url=url,
                status=data.get('–°—Ç–∞—Ç—É—Å', 'active'),
                views_today=data.get('–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è'),
                total_views=data.get('–í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤')
            )
            
            print(f"‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ Cian —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
            return property_data
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Cian: {e}")
            return None
    
    def _extract_cian_data(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã Cian"""
        data = {'URL': url}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—É—é –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        page_text = soup.get_text(" ", strip=True).lower()
        is_blocked = bool(re.search(r"–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –∑–∞–ø—Ä–æ—Å—ã.*–Ω–µ —Ä–æ–±–æ—Ç|–ø–æ—Ö–æ–∂–∏ –Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ", page_text))
        if is_blocked:
            data['–°—Ç–∞—Ç—É—Å'] = None
        elif soup.find(string=re.compile(r"–û–±—ä—è–≤–ª–µ–Ω–∏–µ —Å–Ω—è—Ç–æ", re.IGNORECASE)):
            data['–°—Ç–∞—Ç—É—Å'] = '–°–Ω—è—Ç–æ'
        
        # –ú–µ—Ç–∫–∏
        labels = [span.get_text(strip=True) for span in soup.select('div[data-name="LabelsLayoutNew"] > span span:last-of-type')]
        data['–ú–µ—Ç–∫–∏'] = '; '.join(labels) if labels else None
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
        h1 = soup.find('h1')
        if h1:
            m = re.search(r"(\d+)[^\d]*[-‚Äì]?–∫–æ–º–Ω", h1.get_text())
            if m:
                data['–ö–æ–º–Ω–∞—Ç'] = self._extract_number(m.group(1))
        
        # –¶–µ–Ω–∞
        price_el = (
            soup.select_one('[data-name="NewbuildingPriceInfo"] [data-testid="price-amount"] span')
            or soup.select_one('[data-name="AsideGroup"] [data-testid="price-amount"] span')
        )
        if price_el:
            data['–¶–µ–Ω–∞_raw'] = self._extract_number(price_el.get_text())
            if '–°—Ç–∞—Ç—É—Å' not in data or data['–°—Ç–∞—Ç—É—Å'] is None:
                data['–°—Ç–∞—Ç—É—Å'] = '–ê–∫—Ç–∏–≤–Ω–æ'
        
        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        summary = soup.select_one('[data-name="OfferSummaryInfoLayout"]')
        if summary:
            for item in summary.select('[data-name="OfferSummaryInfoItem"]'):
                ps = item.find_all('p')
                if len(ps) < 2:
                    continue
                key = ps[0].get_text(strip=True)
                val = ps[1].get_text(strip=True)
                kl = key.lower().strip()
                if key == '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è':
                    data[key] = val
                    continue
                if kl == '—ç—Ç–∞–∂': 
                    # –†–∞–∑–±–∏—Ä–∞–µ–º —ç—Ç–∞–∂ –Ω–∞ —Ç–µ–∫—É—â–∏–π –∏ –æ–±—â–∏–π
                    floor_info = self._parse_floor_info(val)
                    data['–≠—Ç–∞–∂'] = floor_info['current_floor']
                    data['–í—Å–µ–≥–æ —ç—Ç–∞–∂–µ–π'] = floor_info['total_floors']
                    continue
                if kl in ['—Å–∞–Ω—É–∑–µ–ª', '–±–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤']:
                    data[key] = val
                    continue
                data[key] = self._extract_number(val) if re.search(r"\d", val) else val
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        cont = soup.find('div', {'data-name': 'ObjectFactoids'})
        if cont:
            lines = cont.get_text(separator='\n', strip=True).split('\n')
            for i in range(0, len(lines)-1, 2):
                key, val = lines[i].strip(), lines[i+1].strip()
                kl = key.lower().strip()
                if key == '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è': 
                    data[key] = val
                    continue
                if kl == '—ç—Ç–∞–∂' and '–≠—Ç–∞–∂' not in data: 
                    # –†–∞–∑–±–∏—Ä–∞–µ–º —ç—Ç–∞–∂ –Ω–∞ —Ç–µ–∫—É—â–∏–π –∏ –æ–±—â–∏–π
                    floor_info = self._parse_floor_info(val)
                    data['–≠—Ç–∞–∂'] = floor_info['current_floor']
                    data['–í—Å–µ–≥–æ —ç—Ç–∞–∂–µ–π'] = floor_info['total_floors']
                elif kl in ['—Å–∞–Ω—É–∑–µ–ª', '–±–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤']: 
                    data[key] = val
                else: 
                    data[key] = self._extract_number(val) if re.search(r"\d", val) else val
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
        stats_re = re.compile(r"([\d\s]+)\s–ø—Ä–æ—Å–º–æ—Ç—Ä\S*,\s*(\d+)\s–∑–∞ —Å–µ–≥–æ–¥–Ω—è,\s*(\d+)\s—É–Ω–∏–∫–∞–ª—å", re.IGNORECASE)
        st = soup.find(string=stats_re)
        if st:
            m = stats_re.search(st)
            data['–í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'], data['–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è'], data['–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'] = (
                self._extract_number(m.group(1)), self._extract_number(m.group(2)), self._extract_number(m.group(3))
            )
        
        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—è –∏ –º–µ—Ç—Ä–æ
        geo = soup.select_one('div[data-name="Geo"]')
        if geo:
            span = geo.find('span', itemprop='name')
            addr = span['content'] if span and span.get('content') else ', '.join(
                a.get_text(strip=True) for a in geo.select('a[data-name="AddressItem"]')
            )
            parts = [s.strip() for s in addr.split(',') if s.strip()]
            data['–ê–¥—Ä–µ—Å'] = ', '.join(parts[-2:]) if len(parts) > 1 else addr
            
            # –ú–µ—Ç—Ä–æ
            stations = []
            for li in geo.select('ul[data-name="UndergroundList"] li[data-name="UndergroundItem"]'):
                st_el = li.find('a', href=True)
                tm_el = li.find('span', class_=re.compile(r".*underground_time.*"))
                if st_el and tm_el:
                    name = st_el.get_text(strip=True)
                    m = re.search(r"(\d+)", tm_el.get_text(strip=True))
                    stations.append((name, int(m.group(1)) if m else None))
            if stations:
                station, time_to = min(stations, key=lambda x: x[1] or float('inf'))
                data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ'] = f"{time_to} {station}"
        
        # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
        data['photo_urls'] = self._extract_cian_photos(soup)
        
        return data
    
    def _parse_floor_info(self, text: str) -> Dict[str, Optional[int]]:
        """–†–∞–∑–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç—Ç–∞–∂–µ –Ω–∞ —Ç–µ–∫—É—â–∏–π –∏ –æ–±—â–∏–π"""
        if not text:
            return {'current_floor': None, 'total_floors': None}
        
        s = str(text).replace('\u00A0', ' ').strip().lower()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω "13 –∏–∑ 37" –∏–ª–∏ "13/37"
        m = re.search(r"(\d+)\s*(?:–∏–∑|/)\s*(\d+)", s)
        if m:
            return {
                'current_floor': int(m.group(1)),
                'total_floors': int(m.group(2))
            }
        
        # –¢–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π —ç—Ç–∞–∂
        m2 = re.search(r"(\d+)\b", s)
        if m2:
            return {
                'current_floor': int(m2.group(1)),
                'total_floors': None
            }
        
        return {'current_floor': None, 'total_floors': None}
    
    def _extract_cian_photos(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —Å Cian"""
        photo_urls = []
        
        try:
            # –ò—â–µ–º –≥–∞–ª–µ—Ä–µ—é
            gallery = soup.find('div', {'data-name': 'GalleryInnerComponent'})
            if not gallery:
                return photo_urls
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = gallery.find_all('img', src=True)
            for img in images:
                src = img.get('src')
                if src and src.startswith('http') and 'cdn-cian.ru' in src:
                    photo_urls.append(src)
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ background-image
            elements_with_bg = gallery.find_all(style=re.compile(r'background-image'))
            for elem in elements_with_bg:
                style = elem.get('style', '')
                bg_match = re.search(r'background-image:\s*url\(["\']?([^"\')\s]+)["\']?\)', style)
                if bg_match:
                    bg_url = bg_match.group(1)
                    if bg_url.startswith('http') and ('cdn-cian.ru' in bg_url or 'kinescopecdn.net' in bg_url):
                        photo_urls.append(bg_url)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
            seen = set()
            unique_photos = []
            for url in photo_urls:
                if url not in seen:
                    seen.add(url)
                    unique_photos.append(url)
            
            return unique_photos
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π Cian: {e}")
            return []
    
    def _extract_number(self, text: str) -> Optional[Union[int, float]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text or text == '‚Äî':
            return None
        cleaned = re.sub(r"[^\d.,]", "", text)
        cleaned = cleaned.replace('\u00A0', '').replace(' ', '').replace(',', '.')
        try:
            return float(cleaned) if '.' in cleaned else int(cleaned)
        except ValueError:
            return None
    
    def cleanup(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã"""
        try:
            self.session.close()
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Ä–µ—Å—É—Ä—Å–æ–≤: {e}")
    
    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏"""
        self.cleanup()

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞
parser = RealtyParserAPI()

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="Realty Parser HTTP API",
    description="API –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ —Å Avito –∏ Cian",
    version="1.0.0"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS –¥–ª—è Fastify
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ —É–∫–∞–∂–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API endpoints
@app.post("/api/parse/urls", response_model=ParseResponse)
async def parse_by_urls(request: ParseUrlsRequest):
    """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ —Å–ø–∏—Å–∫—É URL"""
    try:
        properties = await parser.parse_properties_batch(request.urls)
        
        return ParseResponse(
            success=True,
            data=properties,
            total=len(properties),
            message=f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ {len(properties)} –∏–∑ {len(request.urls)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π",
            timestamp=str(datetime.now())
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")

@app.post("/api/parse/text", response_model=ParseResponse)
async def parse_from_text(request: ParseTextRequest):
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    try:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑ —Ç–µ–∫—Å—Ç–∞
        urls = extract_urls(request.text)
        if not urls:
            raise HTTPException(status_code=400, detail="URL –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ")
        
        # –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        properties = await parser.parse_properties_batch(urls)
        
        return ParseResponse(
            success=True,
            data=properties,
            total=len(properties),
            message=f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(urls)} URL, —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ {len(properties)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π",
            timestamp=str(datetime.now())
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞: {str(e)}")

@app.get("/api/parse/single")
async def parse_single_property(url: str):
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ URL"""
    try:
        property_data = await parser.parse_property(url)
        if property_data:
            return {
                "success": True,
                "data": property_data.to_dict(),
                "message": "–û–±—ä—è–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ"
            }
        else:
            raise HTTPException(status_code=400, detail="–ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")

@app.get("/api/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è API"""
    return {
        "status": "healthy",
        "service": "realty-parser-api",
        "avito_available": parser.is_avito_url("https://avito.ru"),
        "cian_available": parser.is_cian_url("https://cian.ru")
    }

@app.get("/api/sources")
async def get_supported_sources():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    return {
        "sources": [
            {
                "name": "Avito",
                "domain": "avito.ru",
                "available": AVITO_AVAILABLE,
                "source_id": "avito"
            },
            {
                "name": "Cian", 
                "domain": "cian.ru",
                "available": True,
                "source_id": "cian"
            }
        ]
    }

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö Python –º–æ–¥—É–ª—è—Ö)
async def parse_property(url: str) -> Optional[PropertyData]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
    return await parser.parse_property(url)

async def parse_properties_batch(urls: List[str]) -> List[PropertyData]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥"""
    return await parser.parse_properties_batch(urls)

def extract_urls(raw_input: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    return re.findall(r'https?://[^\s,;]+', raw_input)

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
realty_parser = RealtyParserAPI()

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ HTTP —Å–µ—Ä–≤–µ—Ä–∞
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8008,
        log_level="info"
    )
