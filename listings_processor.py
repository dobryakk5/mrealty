import re, json
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook,load_workbook
from openpyxl.styles import Font
from openpyxl.utils import get_column_letter
from io import BytesIO
from typing import List, Dict, Any, Tuple
import asyncio
from datetime import datetime


# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
from db_handler import save_listings, find_similar_ads_grouped, call_update_ad

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º–∏
from photo_processor import photo_processor

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤
HEADERS = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/115.0.0.0 Safari/537.36'
    ),
    'Accept-Language': 'ru-RU,ru;q=0.9',
}

def extract_number(text: str):
    if not text or text == '‚Äî':
        return None
    cleaned = re.sub(r"[^\d.,]", "", text)
    cleaned = cleaned.replace('\u00A0', '').replace(' ', '').replace(',', '.')
    try:
        return float(cleaned) if '.' in cleaned else int(cleaned)
    except ValueError:
        return None

async def export_listings_to_excel(listing_urls: list[str], user_id: int, output_path: str = None) -> tuple[BytesIO, int]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –ë–î –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Excel-—Ñ–∞–π–ª –∏ request_id.
    :param listing_urls: —Å–ø–∏—Å–æ–∫ URL –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    :param user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
    :param output_path: –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –Ω–∞ –¥–∏—Å–∫
    :return: tuple (BytesIO —Å –¥–∞–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞, request_id)
    """
    sess = requests.Session()
    rows = []
    for url in listing_urls:
        try:
            rows.append(parse_listing(url, sess))
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –ø–æ–ª—É—á–∞–µ–º request_id
    request_id = await save_listings(rows, user_id)

    # –§–æ—Ä–º–∏—Ä—É–µ–º DataFrame
    df = pd.DataFrame(rows)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω
    if '–¶–µ–Ω–∞_raw' in df.columns:
        df['–¶–µ–Ω–∞'] = df['–¶–µ–Ω–∞_raw']
        df = df.sort_values('–¶–µ–Ω–∞_raw')
        df.drop('–¶–µ–Ω–∞_raw', axis=1, inplace=True)

    # –ü–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
    ordered = [
        '–ö–æ–º–Ω–∞—Ç', '–¶–µ–Ω–∞', '–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å', '–ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å',
        '–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏', '–°–∞–Ω—É–∑–µ–ª', '–ë–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è', '–í–∏–¥ –∏–∑ –æ–∫–æ–Ω',
        '–†–µ–º–æ–Ω—Ç', '–≠—Ç–∞–∂', '–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏', '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è',
        '–¢–∏–ø –¥–æ–º–∞', '–¢–∏–ø –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤', '–ü–∞—Ä–∫–æ–≤–∫–∞',
        '–ü–æ–¥—ä–µ–∑–¥—ã', '–û—Ç–æ–ø–ª–µ–Ω–∏–µ', '–ê–≤–∞—Ä–∏–π–Ω–æ—Å—Ç—å', '–ì–∞–∑–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ',
        '–í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤', '–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è', '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤',
        '–ê–¥—Ä–µ—Å', '–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ', '–ú–µ—Ç–∫–∏', '–°—Ç–∞—Ç—É—Å', '–¢–∏–ø –∂–∏–ª—å—è', 'URL'
    ]
    df = df[[c for c in ordered if c in df.columns]]

    # –ó–∞–ø–∏—Å—å –≤ BytesIO
    bio = BytesIO()
    df.to_excel(bio, index=False)
    bio.seek(0)

    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å—Ç–æ–ª–±–µ—Ü '–¶–µ–Ω–∞'
    if output_path:
        with open(output_path, 'wb') as f:
            f.write(bio.getbuffer())

        wb = load_workbook(output_path)
        ws = wb.active
        # –í—ã–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        for cell in ws[1]:
            cell.font = Font(bold=True)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É '–¶–µ–Ω–∞' –∏ –∑–∞–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç —Ç—ã—Å—è—á
        price_idx = df.columns.get_loc('–¶–µ–Ω–∞') + 1
        price_col = get_column_letter(price_idx)
        custom_format = '#,##0'
        for row in range(2, ws.max_row + 1):
            cell = ws[f"{price_col}{row}"]
            if isinstance(cell.value, (int, float)):
                cell.number_format = custom_format

        wb.save(output_path)

    return bio, request_id
# –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è

def parse_listing(url: str, session: requests.Session) -> dict:
    resp = session.get(url, headers=HEADERS)
    resp.encoding = 'utf-8'
    soup = BeautifulSoup(resp.text, 'html.parser')

    data = {'URL': url}
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—É—é –±–ª–æ–∫–∏—Ä–æ–≤–∫—É (–∫–∞–ø—á–∞/–∞–Ω—Ç–∏–±–æ—Ç)
    page_text = soup.get_text(" ", strip=True).lower()
    is_blocked = bool(re.search(r"–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –∑–∞–ø—Ä–æ—Å—ã.*–Ω–µ —Ä–æ–±–æ—Ç|–ø–æ—Ö–æ–∂–∏ –Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ", page_text))
    if is_blocked:
        data['–°—Ç–∞—Ç—É—Å'] = None
    elif soup.find(string=re.compile(r"–û–±—ä—è–≤–ª–µ–Ω–∏–µ —Å–Ω—è—Ç–æ", re.IGNORECASE)):
        data['–°—Ç–∞—Ç—É—Å'] = '–°–Ω—è—Ç–æ'
    labels = [span.get_text(strip=True) for span in soup.select('div[data-name="LabelsLayoutNew"] > span span:last-of-type')]
    data['–ú–µ—Ç–∫–∏'] = '; '.join(labels) if labels else None
    h1 = soup.find('h1')
    if h1:
        m = re.search(r"(\d+)[^\d]*[-‚Äì]?–∫–æ–º–Ω", h1.get_text())
        if m:
            data['–ö–æ–º–Ω–∞—Ç'] = extract_number(m.group(1))
    price_el = (
        soup.select_one('[data-name="NewbuildingPriceInfo"] [data-testid="price-amount"] span')
        or soup.select_one('[data-name="AsideGroup"] [data-testid="price-amount"] span')
    )
    if price_el:
        data['–¶–µ–Ω–∞_raw'] = extract_number(price_el.get_text())
        # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –µ—â—ë –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω –∏ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ü–µ–Ω—É ‚Äî —Å—á–∏—Ç–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã–º
        if '–°—Ç–∞—Ç—É—Å' not in data or data['–°—Ç–∞—Ç—É—Å'] is None:
            data['–°—Ç–∞—Ç—É—Å'] = '–ê–∫—Ç–∏–≤–Ω–æ'

    summary = soup.select_one('[data-name="OfferSummaryInfoLayout"]')
    if summary:
        for item in summary.select('[data-name="OfferSummaryInfoItem"]'):
            ps = item.find_all('p')
            if len(ps) < 2:
                continue
            key = ps[0].get_text(strip=True)
            val = ps[1].get_text(strip=True)
            kl = key.lower().strip()
            if key == '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è':
                data[key] = val
                continue
            if kl == '—ç—Ç–∞–∂': data['–≠—Ç–∞–∂'] = val; continue
            if kl in ['—Å–∞–Ω—É–∑–µ–ª', '–±–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤']:
                data[key] = val; continue
            data[key] = extract_number(val) if re.search(r"\d", val) else val

    cont = soup.find('div', {'data-name': 'ObjectFactoids'})
    if cont:
        lines = cont.get_text(separator='\n', strip=True).split('\n')
        for i in range(0, len(lines)-1, 2):
            key, val = lines[i].strip(), lines[i+1].strip()
            kl = key.lower().strip()
            if key == '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ä–∏—è': data[key] = val; continue
            if kl == '—ç—Ç–∞–∂' and '–≠—Ç–∞–∂' not in data: data['–≠—Ç–∞–∂'] = val
            elif kl in ['—Å–∞–Ω—É–∑–µ–ª', '–±–∞–ª–∫–æ–Ω/–ª–æ–¥–∂–∏—è', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ñ—Ç–æ–≤']: data[key] = val
            else: data[key] = extract_number(val) if re.search(r"\d", val) else val

    stats_re = re.compile(r"([\d\s]+)\s–ø—Ä–æ—Å–º–æ—Ç—Ä\S*,\s*(\d+)\s–∑–∞ —Å–µ–≥–æ–¥–Ω—è,\s*(\d+)\s—É–Ω–∏–∫–∞–ª—å", re.IGNORECASE)
    st = soup.find(string=stats_re)
    if st:
        m = stats_re.search(st)
        data['–í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'], data['–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è'], data['–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'] = (
            extract_number(m.group(1)), extract_number(m.group(2)), extract_number(m.group(3))
        )

    geo = soup.select_one('div[data-name="Geo"]')
    if geo:
        span = geo.find('span', itemprop='name')
        addr = span['content'] if span and span.get('content') else ', '.join(
            a.get_text(strip=True) for a in geo.select('a[data-name="AddressItem"]')
        )
        parts = [s.strip() for s in addr.split(',') if s.strip()]
        data['–ê–¥—Ä–µ—Å'] = ', '.join(parts[-2:]) if len(parts) > 1 else addr
        stations = []
        for li in geo.select('ul[data-name="UndergroundList"] li[data-name="UndergroundItem"]'):
            st_el = li.find('a', href=True)
            tm_el = li.find('span', class_=re.compile(r".*underground_time.*"))
            if st_el and tm_el:
                name = st_el.get_text(strip=True)
                m = re.search(r"(\d+)", tm_el.get_text(strip=True))
                stations.append((name, int(m.group(1)) if m else None))
        if stations:
            station, time_to = min(stations, key=lambda x: x[1] or float('inf'))
            data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ'] = f"{time_to} {station}"

    return data


def extract_urls(raw_input: str) -> tuple[list[str], int]:
    urls = re.findall(r'https?://[^\s,;]+', raw_input)
    return urls, len(urls)


async def check_and_update_ad_from_url(url: str, current_price: Any = None, current_is_active: Any = None) -> Dict[str, Any] | None:
    """
    –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ cian.ru ‚Äî –ø–∞—Ä—Å–∏—Ç –µ—ë, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∏ —Ü–µ–Ω—É —Å —Å–∞–π—Ç–∞.
    –û–±–Ω–æ–≤–ª—è–µ—Ç –ë–î –≤—ã–∑–æ–≤–æ–º CALL users.update_ad(p_price, p_is_actual, p_code, p_url_id):
      - p_is_actual –≤—Å–µ–≥–¥–∞ –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: 1 –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏, 0 –µ—Å–ª–∏ –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω–æ.
      - p_price –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ (–∏–Ω–∞—á–µ NULL, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏ (url, price, is_active), –ª–∏–±–æ None –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–µ cian.ru.
    """
    if "cian.ru" not in url:
        return None

    # –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å —Å–∞–π—Ç–∞
    try:
        session = requests.Session()
        parsed = parse_listing(url, session)
    except Exception:
        # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è ‚Äî –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ–±—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å
        return None

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–∂–∏–¥–∞–µ–º—ã–º –∫–ª—é—á–∞–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è/–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    new_price_raw = parsed.get("–¶–µ–Ω–∞_raw") or parsed.get("–¶–µ–Ω–∞")
    try:
        new_price = int(new_price_raw) if new_price_raw is not None else None
    except Exception:
        new_price = None
    new_status_text = parsed.get("–°—Ç–∞—Ç—É—Å")
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω
    new_is_active_bool = None
    if isinstance(new_status_text, str):
        s = new_status_text.strip().lower()
        inactive_markers = ("—Å–Ω—è—Ç–æ", "—Å–Ω—è—Ç", "–Ω–µ–∞–∫—Ç–∏–≤", "—É–¥–∞–ª–µ–Ω", "—É–¥–∞–ª—ë–Ω", "–Ω–µ—Ç –≤ –ø—Ä–æ–¥–∞–∂–µ")
        new_is_active_bool = not any(m in s for m in inactive_markers)
    new_is_actual = (1 if new_is_active_bool else 0) if new_is_active_bool is not None else None

    # –ò–∑–≤–ª–µ–∫–∞–µ–º url_id ‚Äî –ø–æ—Å–ª–µ–¥–Ω—è—è —á–∏—Å–ª–æ–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å—Å—ã–ª–∫–µ (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ query/anchor)
    digits = re.findall(r"/(\d+)(?:/|$)", url)
    url_id = int(digits[-1]) if digits else None

    # –§–æ—Ä–º–∏—Ä—É–µ–º JSON –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ –ë–î
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∏–∑–º–µ–Ω—è–ª–∞—Å—å –ª–∏ —Ü–µ–Ω–∞; —Å—Ç–∞—Ç—É—Å (is_actual) –≤—Å–µ–≥–¥–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –ë–î –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    price_changed = False
    curr_price_int = None
    if current_price is not None:
        try:
            curr_price_int = int(current_price)
        except Exception:
            curr_price_int = None
    if curr_price_int is not None and new_price is not None and curr_price_int != int(new_price):
        price_changed = True
    # current_is_active –º–æ–∂–µ—Ç –±—ã—Ç—å True/False –ª–∏–±–æ 1/0 ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã

    # –ï—Å–ª–∏ url_id –Ω–∞–π–¥–µ–Ω –∏ –µ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è ‚Äî –≤—ã–∑—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –ë–î
    if url_id is not None:
        try:
            await call_update_ad(new_price if price_changed else None, new_is_actual, 4, url_id)
        except Exception as e:
            print(f"[DEBUG] call_update_ad failed for url_id={url_id}: {e}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
    result = {"url": url, "price": new_price}
    if new_is_active_bool is not None:
        result["is_active"] = new_is_active_bool
    return result

async def export_sim_ads(
    request_id: int,
    output_path: str = None
) -> Tuple[BytesIO, int]:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç Excel –≤–∏–¥–∞:
      –ê–¥—Ä–µ—Å1
      –°—Å—ã–ª–∫–∞ | –¶–µ–Ω–∞ | –ö–æ–º–Ω–∞—Ç | –°–æ–∑–¥–∞–Ω–æ | –û–±–Ω–æ–≤–ª–µ–Ω–æ | –ê–∫—Ç–∏–≤–Ω–æ | –í–ª–∞–¥–µ–ª–µ—Ü
      ... —Å "–ê–∫—Ç–∏–≤–Ω–æ" = –¥–∞/–Ω–µ—Ç, –¥–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY ...
    –ê–≤—Ç–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
    """
    groups: List[Dict[str, Any]] = await find_similar_ads_grouped(request_id)

    wb = Workbook()
    ws = wb.active
    ws.title = "–ü–æ—Ö–æ–∂–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"
    bold = Font(bold=True)

    # –ü–æ—Ä—è–¥–æ–∫ –∫–ª—é—á–µ–π –∏ –∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∏
    ordered_keys = ["url", "price", "rooms", "created", "updated", "is_active", "person_type"]
    header_map = {
        "url": "–°—Å—ã–ª–∫–∞",
        "price": "–¶–µ–Ω–∞",
        "rooms": "–ö–æ–º–Ω–∞—Ç",
        "created": "–°–æ–∑–¥–∞–Ω–æ",
        "updated": "–û–±–Ω–æ–≤–ª–µ–Ω–æ",
        "is_active": "–ê–∫—Ç–∏–≤–Ω–æ",
        "person_type": "–í–ª–∞–¥–µ–ª–µ—Ü",
    }

    for grp in groups:
        addr = grp.get("address", "")
        ads_raw = grp.get("ads", [])

        # ‚Äî –ê–¥—Ä–µ—Å –∂–∏—Ä–Ω—ã–º
        ws.append([addr])
        for cell in ws[ws.max_row]:
            cell.font = bold

        # ‚Äî –†–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞
        if isinstance(ads_raw, str):
            try:
                ads_raw = json.loads(ads_raw)
            except json.JSONDecodeError:
                ads_raw = []

        # ‚Äî –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ dict
        ads: List[Dict[str, Any]] = []
        for ad in ads_raw:
            if isinstance(ad, dict):
                ads.append(ad)
            else:
                try:
                    ads.append(dict(ad))
                except Exception:
                    continue

        if not ads:
            ws.append([])  # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
            continue

        # ‚Äî –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–æ–≤
        headers = [header_map[k] for k in ordered_keys]
        ws.append(headers)
        for cell in ws[ws.max_row]:
            cell.font = bold

        # ‚Äî –î–∞–Ω–Ω—ã–µ –ø–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∏ –≤–æ–∑–º–æ–∂–Ω—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –∏–∑ —Å–∞–π—Ç–∞ –¥–ª—è —Å—Å—ã–ª–æ–∫ cian)
        for ad in ads:
            # DEBUG: –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            try:
                dbg_url = ad.get("url")
                dbg_price = ad.get("price")
                dbg_is_active = ad.get("is_active")
                print(f"[DEBUG] check ad url={dbg_url!r} price={dbg_price!r} is_active={dbg_is_active!r}")
            except Exception:
                pass
            # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ ‚Äî –Ω–∞ cian.ru, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –≤ –ë–î
            url_val = ad.get("url")
            if isinstance(url_val, str) and "cian.ru" in url_val:
                updated = await check_and_update_ad_from_url(
                    url_val,
                    current_price=ad.get("price"),
                    current_is_active=ad.get("is_active"),
                )
                if updated:
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —Ç–µ–º, —á—Ç–æ –ø—Ä–∏—à–ª–æ –∏–∑ –ë–î, –∏ –ø—Ä–∏ –æ—Ç–ª–∏—á–∏–∏ ‚Äî –ø–æ–¥–º–µ–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ç–µ–∫—É—â–µ–º –∫–æ—Ä—Ç–µ–∂–µ
                    if updated.get("price") is not None and updated.get("price") != ad.get("price"):
                        ad["price"] = updated.get("price")
                    # –í–ê–ñ–ù–û: —Å—Ç–∞—Ç—É—Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ Excel –±–µ—Ä—ë–º —Å —Å–∞–π—Ç–∞, –∞ –Ω–µ –∏–∑ –ë–î
                    if "is_active" in updated:
                        ad["is_active"] = bool(updated.get("is_active"))

            row = []
            for k in ordered_keys:
                val = ad.get(k, "")
                if k == "is_active":
                    row.append("–¥–∞" if val else "–Ω–µ—Ç")
                elif k in ("created", "updated") and isinstance(val, str):
                    # –ø—Ä–∏–≤–æ–¥–∏–º YYYY-MM-DDTHH:MM:SS –∫ DD.MM.YYYY
                    date_part = val.split('T')[0]
                    parts = date_part.split('-')
                    if len(parts) == 3:
                        row.append(f"{parts[2]}.{parts[1]}.{parts[0]}")
                    else:
                        row.append(val)
                else:
                    row.append(val)
            ws.append(row)

        ws.append([])  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏

    # –ê–≤—Ç–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫
    for column_cells in ws.columns:
        max_length = max(
            len(str(cell.value)) if cell.value is not None else 0
            for cell in column_cells
        )
        col_letter = get_column_letter(column_cells[0].column)
        ws.column_dimensions[col_letter].width = max_length + 2

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ BytesIO
    bio = BytesIO()
    wb.save(bio)
    bio.seek(0)

    if output_path:
        with open(output_path, "wb") as f:
            f.write(bio.getbuffer())

    return bio, request_id

def extract_photo_urls(soup: BeautifulSoup) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–∑ –≥–∞–ª–µ—Ä–µ–∏ CIAN
    """
    photo_urls = []
    
    try:
        # –ò—â–µ–º –≥–∞–ª–µ—Ä–µ—é –ø–æ data-name="GalleryInnerComponent"
        gallery = soup.find('div', {'data-name': 'GalleryInnerComponent'})
        if not gallery:
            return photo_urls
        
        # –ò—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –≥–∞–ª–µ—Ä–µ–µ
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = gallery.find_all('img', src=True)
        for img in images:
            src = img.get('src')
            if src and src.startswith('http') and 'cdn-cian.ru' in src:
                photo_urls.append(src)
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ background-image (–¥–ª—è –≤–∏–¥–µ–æ –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ñ–æ—Ç–æ)
        elements_with_bg = gallery.find_all(style=re.compile(r'background-image'))
        for elem in elements_with_bg:
            style = elem.get('style', '')
            bg_match = re.search(r'background-image:\s*url\(["\']?([^"\')\s]+)["\']?\)', style)
            if bg_match:
                bg_url = bg_match.group(1)
                if bg_url.startswith('http') and ('cdn-cian.ru' in bg_url or 'kinescopecdn.net' in bg_url):
                    photo_urls.append(bg_url)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_photos = []
        for url in photo_urls:
            if url not in seen:
                seen.add(url)
                unique_photos.append(url)
        
        return unique_photos
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: {e}")
        return []

def generate_html_gallery(listing_urls: list[str], user_id: int, subtitle: str = None) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π HTML –¥–æ–∫—É–º–µ–Ω—Ç —Å –≥–∞–ª–µ—Ä–µ–µ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    """
    sess = requests.Session()
    html_parts = []
    
    html_parts.append("""
    <!DOCTYPE html>
    <html lang="ru">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>–ü–æ–¥–±–æ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
            .listing { background: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            .listing h3 { color: #333; margin-top: 0; margin-bottom: 15px; }
            .listing p { margin: 8px 0; color: #555; }
            .listing strong { color: #333; }
            .main-title { color: #333; margin-bottom: 10px; }
            .subtitle { color: #666; font-size: 18px; margin-bottom: 30px; font-style: italic; }
            .photo-grid { 
                display: grid; 
                grid-template-columns: repeat(auto-fill, minmax(180px, 1fr)); 
                gap: 8px; 
                margin: 15px 0; 
                max-height: 600px; 
                overflow-y: auto; 
                padding: 10px;
                border: 1px solid #eee;
                border-radius: 8px;
            }
            .photo-item { position: relative; }
            .photo-item img { 
                width: 100%; 
                height: 140px; 
                object-fit: cover; 
                border-radius: 5px; 
                border: 2px solid transparent;
                transition: border-color 0.2s;
                background: #f8f9fa;
            }
            .photo-item img:hover { 
                border-color: #0066cc;
            }
            .photo-fallback { 
                width: 100%; 
                height: 140px; 
                display: flex; 
                flex-direction: column; 
                justify-content: center; 
                align-items: center;
            }
            .no-photos { color: #666; font-style: italic; }
            .photo-info { 
                background: #f8f9fa; 
                padding: 15px; 
                border-radius: 8px; 
                margin-top: 15px; 
                font-size: 14px;
                border-left: 4px solid #0066cc;
            }
            .photo-info strong { color: #333; }
            .photo-info small { line-height: 1.4; }
            
            /* –ú–æ–±–∏–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è */
            @media (max-width: 768px) {
                body { margin: 10px; }
                .listing { padding: 15px; margin: 15px 0; }
                .photo-grid { 
                    grid-template-columns: repeat(auto-fill, minmax(150px, 1fr)); 
                    gap: 6px; 
                    padding: 8px;
                }
                .photo-item img, .photo-fallback { 
                    height: 120px; 
                }
                .main-title { font-size: 24px; }
                .subtitle { font-size: 16px; }
            }
        </style>
    </head>
    <body>
        <h1 class="main-title">üè† –ü–æ–¥–±–æ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏</h1>
    """)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if subtitle and subtitle.strip():
        html_parts.append(f'<h2 class="subtitle">{subtitle.strip()}</h2>')
    
    for i, url in enumerate(listing_urls, 1):
        try:
            # –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            listing_data = parse_listing(url, sess)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            soup = BeautifulSoup(requests.get(url, headers=HEADERS).text, 'html.parser')
            photo_urls = extract_photo_urls(soup)
            
            html_parts.append(f"""
            <div class="listing">
                <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
            """)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if '–ö–æ–º–Ω–∞—Ç' in listing_data and listing_data['–ö–æ–º–Ω–∞—Ç']:
                html_parts.append(f"<p><strong>–ö–æ–º–Ω–∞—Ç:</strong> {listing_data['–ö–æ–º–Ω–∞—Ç']}</p>")
            if '–¶–µ–Ω–∞_raw' in listing_data and listing_data['–¶–µ–Ω–∞_raw']:
                html_parts.append(f"<p><strong>–¶–µ–Ω–∞:</strong> {listing_data['–¶–µ–Ω–∞_raw']:,} ‚ÇΩ</p>")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç–∞–∂/—ç—Ç–∞–∂–Ω–æ—Å—Ç—å
            if '–≠—Ç–∞–∂' in listing_data and listing_data['–≠—Ç–∞–∂']:
                html_parts.append(f"<p><strong>–≠—Ç–∞–∂:</strong> {listing_data['–≠—Ç–∞–∂']}</p>")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∞–∂ –æ–±—â–∏–π
            if '–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å' in listing_data and listing_data['–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å']:
                html_parts.append(f"<p><strong>–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å:</strong> {listing_data['–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å']} –º¬≤</p>")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫—É—Ö–Ω—é
            if '–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏' in listing_data and listing_data['–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏']:
                html_parts.append(f"<p><strong>–ö—É—Ö–Ω—è:</strong> {listing_data['–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏']} –º¬≤</p>")
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º "–ú–µ—Ç—Ä–æ" –≤ "–ú–∏–Ω—É—Ç –¥–æ –º–µ—Ç—Ä–æ"
            if '–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ' in listing_data and listing_data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ']:
                html_parts.append(f"<p><strong>–ú–∏–Ω—É—Ç –¥–æ –º–µ—Ç—Ä–æ:</strong> {listing_data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ']}</p>")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            if photo_urls:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–µ—Ç–∫—É —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (–≤—Å–µ —Ñ–æ—Ç–æ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
                photo_grid_html = photo_processor.generate_photo_grid_html([], 'url')
                
                # –ó–∞–º–µ–Ω—è–µ–º –ø—É—Å—Ç—É—é —Å–µ—Ç–∫—É –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
                html_parts.append(f'<div class="photo-grid">')
                for j, photo_url in enumerate(photo_urls):
                    html_parts.append(f"""
                    <div class="photo-item">
                        <img src="{photo_url}" alt="–§–æ—Ç–æ {j+1}" 
                             onerror="this.style.display='none'; this.nextElementSibling.style.display='block';"
                             loading="lazy">
                        <div class="photo-fallback" style="display: none; background: #f0f0f0; border: 1px dashed #ccc; border-radius: 5px; padding: 20px; text-align: center; color: #666;">
                            <div>üì∑ –§–æ—Ç–æ {j+1}</div>
                            <div style="font-size: 12px; margin-top: 5px;">
                                <a href="{photo_url}" target="_blank" style="color: #0066cc;">–û—Ç–∫—Ä—ã—Ç—å —Ñ–æ—Ç–æ</a>
                            </div>
                        </div>
                    </div>
                    """)
                html_parts.append('</div>')
            else:
                html_parts.append('<p class="no-photos">üì∑ –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã</p>')
            
            html_parts.append('</div>')
            
        except Exception as e:
            html_parts.append(f"""
            <div class="listing">
                <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
                <p style="color: red;">–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}</p>
            </div>
            """)
    
    html_parts.append("""
    </body>
    </html>
    """)
    
    return ''.join(html_parts)

def generate_html_gallery_embedded(listing_urls: list[str], user_id: int, subtitle: str = None) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –¥–æ–∫—É–º–µ–Ω—Ç —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ base64 –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    """
    sess = requests.Session()
    html_parts = []
    
    html_parts.append("""
    <!DOCTYPE html>
    <html lang="ru">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>–ü–æ–¥–±–æ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
            .listing { background: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            .listing h3 { color: #333; margin-top: 0; margin-bottom: 15px; }
            .listing p { margin: 8px 0; color: #555; }
            .listing strong { color: #333; }
            .main-title { color: #333; margin-bottom: 10px; }
            .subtitle { color: #666; font-size: 18px; margin-bottom: 30px; font-style: italic; }
            .photo-grid { 
                display: grid; 
                grid-template-columns: repeat(auto-fill, minmax(180px, 1fr)); 
                gap: 8px; 
                margin: 15px 0; 
                max-height: 600px; 
                overflow-y: auto; 
                padding: 10px;
                border: 1px solid #eee;
                border-radius: 8px;
            }
            .photo-item { position: relative; }
            .photo-item img { 
                width: 100%; 
                height: 140px; 
                object-fit: cover; 
                border-radius: 5px; 
                border: 2px solid transparent;
                transition: border-color 0.2s;
            }
            .photo-item img:hover { 
                border-color: #0066cc;
            }
            .no-photos { color: #666; font-style: italic; }
            .photo-info { 
                background: #f8f9fa; 
                padding: 15px; 
                border-radius: 8px; 
                margin-top: 15px; 
                font-size: 14px;
                border-left: 4px solid #0066cc;
            }
            .photo-info strong { color: #333; }
            
            /* –ú–æ–±–∏–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è */
            @media (max-width: 768px) {
                body { margin: 10px; }
                .listing { padding: 15px; margin: 15px 0; }
                .photo-grid { 
                    grid-template-columns: repeat(auto-fill, minmax(150px, 1fr)); 
                    gap: 6px; 
                    padding: 8px;
                }
                .photo-item img { 
                    height: 120px; 
                }
                .main-title { font-size: 24px; }
                .subtitle { font-size: 16px; }
            }
        </style>
    </head>
    <body>
        <h1 class="main-title">üè† –ü–æ–¥–±–æ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏</h1>
    """)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if subtitle and subtitle.strip():
        html_parts.append(f'<h2 class="subtitle">{subtitle.strip()}</h2>')
    
    for i, url in enumerate(listing_urls, 1):
        try:
            # –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            listing_data = parse_listing(url, sess)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            soup = BeautifulSoup(requests.get(url, headers=HEADERS).text, 'html.parser')
            photo_urls = extract_photo_urls(soup)
            
            html_parts.append(f"""
            <div class="listing">
                <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
            """)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if '–ö–æ–º–Ω–∞—Ç' in listing_data and listing_data['–ö–æ–º–Ω–∞—Ç']:
                html_parts.append(f"<p><strong>–ö–æ–º–Ω–∞—Ç:</strong> {listing_data['–ö–æ–º–Ω–∞—Ç']}</p>")
            if '–¶–µ–Ω–∞_raw' in listing_data and listing_data['–¶–µ–Ω–∞_raw']:
                html_parts.append(f"<p><strong>–¶–µ–Ω–∞:</strong> {listing_data['–¶–µ–Ω–∞_raw']:,} ‚ÇΩ</p>")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç–∞–∂/—ç—Ç–∞–∂–Ω–æ—Å—Ç—å
            if '–≠—Ç–∞–∂' in listing_data and listing_data['–≠—Ç–∞–∂']:
                html_parts.append(f"<p><strong>–≠—Ç–∞–∂:</strong> {listing_data['–≠—Ç–∞–∂']}</p>")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∞–∂ –æ–±—â–∏–π
            if '–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å' in listing_data and listing_data['–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å']:
                html_parts.append(f"<p><strong>–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å:</strong> {listing_data['–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å']} –º¬≤</p>")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫—É—Ö–Ω—é
            if '–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏' in listing_data and listing_data['–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏']:
                html_parts.append(f"<p><strong>–ö—É—Ö–Ω—è:</strong> {listing_data['–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏']} –º¬≤</p>")
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º "–ú–µ—Ç—Ä–æ" –≤ "–ú–∏–Ω—É—Ç –¥–æ –º–µ—Ç—Ä–æ"
            if '–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ' in listing_data and listing_data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ']:
                html_parts.append(f"<p><strong>–ú–∏–Ω—É—Ç –¥–æ –º–µ—Ç—Ä–æ:</strong> {listing_data['–ú–∏–Ω—É—Ç –º–µ—Ç—Ä–æ']}</p>")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤ base64)
            if photo_urls:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –¥–ª—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ HTML (–≤—Å–µ —Ñ–æ—Ç–æ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
                processed_photos = photo_processor.process_photos_for_embedded_html(photo_urls)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–µ—Ç–∫—É —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
                photo_grid_html = photo_processor.generate_photo_grid_html(processed_photos, 'embedded')
                html_parts.append(photo_grid_html)
            else:
                html_parts.append('<p class="no-photos">üì∑ –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã</p>')
            
            html_parts.append('</div>')
            
        except Exception as e:
            html_parts.append(f"""
            <div class="listing">
                <h3>–í–∞—Ä–∏–∞–Ω—Ç #{i}</h3>
                <p style="color: red;">–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}</p>
            </div>
            """)
    
    html_parts.append("""
    </body>
    </html>
    """)
    
    return ''.join(html_parts)

if __name__ == '__main__':
    user_id = 12345
    urls = ["https://www.cian.ru/sale/flat/318826533/"]
    excel = asyncio.run(export_listings_to_excel(urls, user_id, output_path="listings_numeric.xlsx"))
    print("listings_numeric.xlsx —Å–æ–∑–¥–∞–Ω.")
